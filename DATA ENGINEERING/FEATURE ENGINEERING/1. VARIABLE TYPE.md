- [VARIABLE](#variable)
- [NUMERICAL](#numerical)
- [CATEGORICAL](#categorical)
- [DATETIME](#datetime)
- [MIXED](#mixed)

# VARIABLE
A variable is any characteristic, number, or quantity
that can be measured or counted

<a id='numerical'></a>
# NUMERICAL

Numerical variables represent quantitative data and are a critical part of feature engineering. These variables can take numerical values that allow mathematical operations. They are generally divided into two types:

---

## 1. Discrete Numerical Variables
- **Definition**: Variables that represent countable, finite values. These often arise from counting objects or events.
- **Examples**:
  - Number of items purchased (`5`, `10`, `20`).
  - Number of children in a household (`0`, `1`, `2`).
  - Age in years (`21`, `45`, `60`).

### Key Characteristics
- Values are integers.
- Cannot take fractional or decimal values.
- Typically represent **counts** or **categorical encodings**.

### Feature Engineering Tips
- **Binning**: Group discrete values into categories (e.g., `age` into age groups: `18-25`, `26-35`).
- **One-Hot Encoding**: When the discrete variable represents categories (e.g., number of bedrooms), encode each value as a separate binary column.
- **Scaling**: Apply techniques like Min-Max scaling or Standardization to bring the values within a specific range.

---

## 2. Continuous Numerical Variables
- **Definition**: Variables that can take any value within a given range and often arise from measurements.
- **Examples**:
  - Temperature (`20.5°C`, `36.7°C`).
  - Income (`50000.75 USD`, `120000.10 USD`).
  - Height (`170.2 cm`, `185.5 cm`).

### Key Characteristics
- Values can include decimals or fractions.
- Typically represent **measurements** or **ratios**.
- Can take an infinite number of possible values within a range.

### Feature Engineering Tips
- **Normalization/Scaling**: Scale continuous variables for consistency, especially if using models sensitive to magnitude (e.g., linear regression, SVM).
- **Log Transformation**: Apply to variables with a skewed distribution (e.g., income, population growth).
- **Polynomial Features**: Generate higher-order terms (e.g., `x²`, `x³`) to capture non-linear relationships.
- **Discretization**: Convert continuous variables into discrete bins (e.g., temperature ranges: `Cold`, `Moderate`, `Hot`).

---

## General Feature Engineering Techniques for Numerical Variables

1. **Outlier Handling**:
   - Identify outliers using statistical methods (e.g., Z-score, IQR).
   - Treat outliers by capping, clipping, or using transformations (e.g., log).

2. **Interaction Terms**:
   - Combine numerical variables to create new features (e.g., `price_per_unit = total_price / quantity`).

3. **Missing Value Imputation**:
   - Fill missing numerical values using:
     - Mean or median imputation.
     - Predictive imputation based on other variables.

4. **Feature Scaling**:
   - Ensure numerical variables are on the same scale using:
     - **Min-Max Scaling**: Rescales to a range `[0, 1]`.
     - **Standardization**: Converts to a standard normal distribution (mean = 0, standard deviation = 1).

5. **Dimensionality Reduction**:
   - Use techniques like PCA to reduce the dimensionality of numerical features while retaining variance.

6. **Time-Derived Features**:
   - Extract additional information from continuous time variables (e.g., `time_since_last_purchase`, `hour_of_day`).

---

## Summary Table

| **Type**       | **Definition**                           | **Examples**               | **Feature Engineering**                                                                 |
|-----------------|-----------------------------------------|----------------------------|---------------------------------------------------------------------------------------|
| **Discrete**    | Countable, finite values.               | `2 children`, `5 products` | Binning, one-hot encoding, scaling, treating as categories.                          |
| **Continuous**  | Measurable values with decimals/fractions. | `175.5 cm`, `45000 USD`    | Scaling, normalization, log transformation, discretization, outlier handling.        |

By understanding and applying these techniques, numerical variables can be transformed into features that better capture relationships in the data and improve model performance.

<a id='categorical'></a>
# CATEGORICAL

Categorical variables represent qualitative data, typically used to describe labels, groups, or categories. These variables can be classified into two main types: **nominal** and **ordinal**. In addition, special cases like high-cardinality and text-based categories require specific handling.

---

## 1. Nominal Categorical Variables
- **Definition**: Variables that represent categories without any inherent order or ranking.
- **Examples**:
  - Colors (`Red`, `Blue`, `Green`).
  - Cities (`New York`, `London`, `Tokyo`).
  - Product Types (`Electronics`, `Furniture`, `Clothing`).

### Key Characteristics
- Categories are mutually exclusive.
- No implied hierarchy or order between the categories.

### Feature Engineering Tips
- **One-Hot Encoding**: Convert each category into a binary column (e.g., `Red` → `1, 0, 0` for `[Red, Blue, Green]`).
- **Label Encoding**: Assign an arbitrary numerical value to each category (`Red = 0`, `Blue = 1`, `Green = 2`).
- **Frequency Encoding**: Replace categories with their frequency or proportion in the dataset.
- **Hash Encoding**: Useful for high-cardinality data, maps categories to fixed-length integers using a hash function.

---

## 2. Ordinal Categorical Variables
- **Definition**: Variables that represent categories with an inherent order or ranking.
- **Examples**:
  - Education Level (`High School < Bachelor's < Master's < PhD`).
  - Customer Ratings (`Low < Medium < High`).
  - Size (`Small < Medium < Large`).

### Key Characteristics
- Categories have a meaningful order.
- Distance between categories may not be uniform.

### Feature Engineering Tips
- **Ordinal Encoding**: Map categories to integers based on their order (`Low = 1`, `Medium = 2`, `High = 3`).
- **Custom Mapping**: Assign weights based on domain knowledge or business context (e.g., `Small = 0.5`, `Medium = 1.0`, `Large = 1.5`).
- **Polynomial Transformation**: Create interaction terms or non-linear relationships for ordinal features.

---

## 3. Special Cases in Categorical Variables

### High-Cardinality Categorical Variables
- **Definition**: Variables with a large number of unique categories.
- **Examples**:
  - ZIP codes.
  - Product IDs.
  - User IDs.

**Challenges**:
- One-hot encoding can result in a sparse and computationally expensive dataset.

**Solutions**:
- **Target Encoding**: Replace categories with the mean of the target variable (e.g., average sales by product category).
- **Cluster Encoding**: Group similar categories based on patterns in the data.
- **Dimensionality Reduction**: Use techniques like PCA on encoded categories to reduce their size.

---

### Text-Based Categorical Variables
- **Definition**: Variables containing unstructured text data.
- **Examples**:
  - Product descriptions.
  - Review comments.
  - Job titles.

**Solutions**:
- **Text Vectorization**: Convert text to numerical representations using techniques like TF-IDF or word embeddings (e.g., Word2Vec, BERT).
- **Clustering**: Group similar categories based on textual similarity (e.g., using cosine similarity or K-Means clustering).

---

## General Feature Engineering Techniques for Categorical Variables

1. **Imputation of Missing Values**:
   - Replace missing categories with:
     - `Unknown` or `Other`.
     - The most frequent category.

2. **Combining Rare Categories**:
   - Group infrequent categories into an `Other` category to reduce sparsity.

3. **Interaction Features**:
   - Create new features by combining categorical variables (e.g., `Country_City = Country + City`).

4. **Dummy Variable Trap**:
   - When using one-hot encoding, drop one category to avoid multicollinearity (e.g., for `[Red, Blue, Green]`, use only `Red` and `Blue`).

5. **Scalability**:
   - For high-cardinality data, use scalable encodings like target or frequency encoding.

---

## Summary Table

| **Type**                | **Definition**                            | **Examples**                     | **Feature Engineering**                                                                 |
|--------------------------|------------------------------------------|-----------------------------------|---------------------------------------------------------------------------------------|
| **Nominal**              | Categories without order.                | `Colors`, `Cities`, `Products`   | One-hot encoding, frequency encoding, hash encoding.                                  |
| **Ordinal**              | Categories with an inherent order.       | `Education`, `Size`, `Ratings`   | Ordinal encoding, custom mapping, interaction terms.                                  |
| **High-Cardinality**     | Large number of unique categories.       | `ZIP Codes`, `Product IDs`       | Target encoding, cluster encoding, dimensionality reduction.                          |
| **Text-Based**           | Categories with unstructured text.       | `Descriptions`, `Comments`       | Text vectorization, clustering, embedding techniques.                                 |

By carefully engineering categorical variables, you can capture meaningful relationships in your data and improve model performance significantly.

<a id='datetime'></a>
# DATETIME

Date variables represent temporal data and can provide valuable insights into patterns and trends when engineered effectively. These variables are often categorized as **date-only**, **time-only**, or **datetime**.

---

## 1. Date-Only Variables
- **Definition**: Variables containing information about calendar dates, without reference to specific times.
- **Examples**:
  - `2025-01-01` (YYYY-MM-DD format).
  - `01/01/2025` (MM/DD/YYYY format).

### Key Characteristics
- Includes day, month, and year components.
- Useful for identifying long-term trends or seasonal patterns.

### Feature Engineering Tips
- **Date Components**: Extract useful features like:
  - Year (`2025`), month (`01`), day of the month (`01`).
  - Weekday (`Monday`, `Tuesday`), quarter (`Q1`, `Q2`).
- **Elapsed Time**: Calculate time differences from:
  - A fixed reference date (e.g., `days_since_event`).
  - Other date variables (e.g., `days_between_start_and_end`).
- **Holiday Flags**: Create binary features to indicate whether the date is a holiday or falls in a holiday period.

---

## 2. Time-Only Variables
- **Definition**: Variables containing information about specific times within a day, without reference to a calendar date.
- **Examples**:
  - `14:30:00` (HH:MM:SS format).
  - `2:30 PM` (12-hour clock format).

### Key Characteristics
- Includes hour, minute, and second components.
- Useful for identifying time-of-day patterns or activity cycles.

### Feature Engineering Tips
- **Time Components**: Extract specific parts of the time:
  - Hour (`14`), minute (`30`), second (`00`).
  - Time blocks (e.g., `Morning`, `Afternoon`, `Evening`).
- **Conversion**:
  - Convert time to seconds or minutes since midnight for numerical modeling.
- **Periodicity**: Create features that capture cyclical patterns:
  - Sine and cosine transformations for hours to represent time cyclically (e.g., `sin(2π * hour / 24)`).

---

## 3. Datetime Variables
- **Definition**: Variables that combine date and time information into a single entity.
- **Examples**:
  - `2025-01-01 14:30:00` (YYYY-MM-DD HH:MM:SS format).
  - `01/01/2025 2:30 PM` (MM/DD/YYYY HH:MM AM/PM format).

### Key Characteristics
- Combines date and time into one variable.
- Useful for detailed event tracking and real-time analysis.

### Feature Engineering Tips
- **Decomposition**: Break datetime into separate date and time components:
  - Extract features like year, month, day, hour, minute, and second.
- **Derived Features**:
  - Calculate elapsed time from a reference point.
  - Determine time windows (e.g., `within_office_hours`, `after_hours`).
- **Cyclical Features**: Capture cyclical patterns for both dates (e.g., months of the year) and times (e.g., hours of the day).

---

## General Feature Engineering Techniques for Date Variables

1. **Handling Missing Dates**:
   - Replace missing values with placeholders (e.g., `Unknown Date` or `0000-00-00`).
   - Use domain knowledge to impute plausible dates.

2. **Time Zone Normalization**:
   - Standardize datetime variables to a single time zone if dealing with data from multiple regions.

3. **Seasonality**:
   - Create features based on seasonal effects, such as:
     - Season (`Spring`, `Summer`, `Fall`, `Winter`).
     - Fiscal periods (e.g., `Q1`, `Q2`).

4. **Event-Based Features**:
   - Add binary flags for specific events or milestones (e.g., `Black Friday`, `End of Quarter`).

5. **Lag and Lead Features**:
   - Calculate time-lagged or forward-shifted variables for time-series analysis.

6. **Datetime Formatting**:
   - Convert all datetime variables to a uniform format for consistency.

---

## Summary Table

| **Type**         | **Definition**                            | **Examples**                     | **Feature Engineering**                                                                 |
|-------------------|-------------------------------------------|-----------------------------------|---------------------------------------------------------------------------------------|
| **Date-Only**     | Calendar dates without time information.  | `2025-01-01`, `01/01/2025`       | Extract components, calculate elapsed time, holiday flags, seasonal features.        |
| **Time-Only**     | Specific times without date information.  | `14:30:00`, `2:30 PM`            | Extract hour/minute, convert to seconds, create cyclic representations.              |
| **Datetime**      | Combines date and time information.       | `2025-01-01 14:30:00`            | Decompose into date and time parts, calculate elapsed time, time window flags.        |

By engineering date variables effectively, you can capture temporal patterns, trends, and cycles that significantly enhance the predictive power of your models.

<a id='mixed'></a>
# MIXED

Mixed variables are those that combine both numerical and categorical elements, either across observations or within the same observation. These variables require special handling during feature engineering to extract meaningful insights for predictive modeling.

---

## 1. Numbers or Labels in Different Observations
- **Definition**: A variable where some observations are represented as numerical values, while others are represented as categorical labels.
- **Examples**:
  - Age can be a number (`25`, `30`, `45`) or a label (`Young`, `Old`).
  - Price can be a number (`$100`, `$200`) or a range (`Low`, `High`).
  - Ratings may include both numerical scores (`4.5`) and qualitative terms (`Excellent`, `Poor`).

### Key Characteristics
- Observations mix continuous or discrete values with categorical labels.
- May indicate inconsistent data entry or context-specific encoding.

### Feature Engineering Tips
1. **Standardization**:
   - Convert categorical labels to corresponding numerical values using domain knowledge (e.g., `Young = 18-25` → average = `21`).
   - Apply consistent formatting to numerical observations.

2. **Label Encoding**:
   - Map labels to integers (`Young = 1`, `Old = 2`) when there is an implied order.
   - Use one-hot encoding if the labels are nominal (no intrinsic order).

3. **Imputation**:
   - Treat numerical and labeled entries differently when imputing missing values:
     - For numbers, use statistical measures (mean/median).
     - For labels, use mode or frequency-based replacement.

4. **Segmentation**:
   - Split the variable into two separate features:
     - One for numerical values.
     - One for categorical labels.

---

## 2. Numbers and Labels in the Same Observation
- **Definition**: A variable where a single observation contains both numerical and categorical elements.
- **Examples**:
  - Product codes like `A123` (`A` is a category, `123` is a number).
  - Mixed date formats like `January 2025` (`January` is a label, `2025` is a number).
  - Address elements like `Street 45` (`Street` is a label, `45` is a number).

### Key Characteristics
- Individual observations combine text and numeric data.
- Common in unstructured or semi-structured datasets.

### Feature Engineering Tips
1. **Parsing and Splitting**:
   - Separate the variable into distinct features:
     - Extract numeric components (`123`, `2025`, `45`).
     - Extract textual components (`A`, `January`, `Street`).

2. **Feature Transformation**:
   - Convert text components to categorical features using encoding techniques.
   - Process numeric components as continuous or discrete features.

3. **Regular Expressions**:
   - Use regex patterns to extract numbers and labels efficiently:
     - Example: For `A123`, extract `A` and `123` separately.

4. **Domain-Specific Transformation**:
   - If the combination carries meaning, preserve it as a single variable.
   - Example: `A123` could indicate product type `A` and batch `123`. Keep the combination if it's meaningful.

5. **Vectorization for Text**:
   - If the label has significant meaning (e.g., a name or identifier), apply text vectorization (e.g., TF-IDF or embeddings).

---

## General Feature Engineering Techniques for Mixed Variables

1. **Data Cleaning**:
   - Identify and resolve inconsistencies in representation.
   - Example: Convert all observations to a uniform format.

2. **Dimensionality Reduction**:
   - If splitting a variable creates too many features (e.g., one-hot encoding of labels), use dimensionality reduction methods like PCA.

3. **Imputation**:
   - Handle missing values separately for numeric and label components.

4. **Interaction Features**:
   - Create interaction terms between numeric and categorical parts of a variable.

---

## Summary Table

| **Type**                           | **Definition**                                                  | **Examples**                          | **Feature Engineering**                                                                 |
|-------------------------------------|----------------------------------------------------------------|----------------------------------------|---------------------------------------------------------------------------------------|
| **Numbers or Labels in Different Observations** | Some observations are numbers, others are labels.               | `25`, `Young`; `$100`, `Low`          | Standardization, label encoding, imputation, segmentation.                            |
| **Numbers and Labels in the Same Observation**  | A single observation combines numeric and categorical elements. | `A123`, `January 2025`, `Street 45`   | Parsing/splitting, feature transformation, regex extraction, domain-specific encoding. |

By carefully handling mixed variables, you can ensure data consistency, extract meaningful features, and improve the overall performance of your models.
