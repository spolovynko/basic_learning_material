- [INTRO](#intro)
- [STANDARDISATION](#standardisation)
- [MIN MAX SCALING](#min-max-scaling)
- [MEAN NORMALIZATION](#mean)
- [MAX ABS SACLING](#max-abs)
- [ROBUST](#robust)
- [UNIT NORM](#unit-norm)
- [COMPARISON](#comparison)
<a id='intro'></a>
# INTRO

### Feature Scaling: Detailed Overview

#### What is Feature Scaling?
Feature scaling is the process of standardizing the range or distribution of feature values in a dataset. It ensures that features with different scales do not disproportionately influence the model's performance.

---

### Why Does Feature Scaling Matter?

1. **Ensures Fair Weighting**:
   - Models that rely on distance or gradient-based optimization are sensitive to the magnitude of feature values.
   - Without scaling, features with larger ranges can dominate the model's behavior.

2. **Speeds Up Convergence**:
   - Gradient-based algorithms (e.g., gradient descent) converge faster when features are scaled similarly, as it avoids excessively steep or flat gradients.

3. **Improves Model Performance**:
   - Scaling ensures all features contribute equally to the model, preventing bias towards features with higher magnitudes.

4. **Prevents Numerical Instabilities**:
   - Unscaled features can lead to numerical precision issues in computations, especially in large datasets or with high-magnitude features.

---

### Models Susceptible to Feature Scaling

#### 1. **Distance-Based Models**
- **Examples**: K-Nearest Neighbors (KNN), K-Means Clustering, Principal Component Analysis (PCA).
- **Reason**: Distance metrics (e.g., Euclidean) are sensitive to feature magnitudes. Features with larger scales dominate the distance computation.

#### 2. **Gradient-Based Models**
- **Examples**: Logistic Regression, Linear Regression, Neural Networks, Support Vector Machines (SVMs).
- **Reason**: Gradient descent optimization assumes similar scales for all features. Large feature magnitudes can lead to slow or unstable convergence.

#### 3. **Regularization-Based Models**
- **Examples**: Ridge Regression, Lasso Regression.
- **Reason**: Regularization terms (e.g., \( \lambda \sum w^2 \)) penalize weights. Unscaled features can lead to unfair penalties for certain coefficients.

#### 4. **Kernel-Based Models**
- **Examples**: SVMs with RBF kernels.
- **Reason**: Kernel functions compute distances, which are influenced by the feature scales.

---

### Models Less Sensitive to Feature Scaling

- **Tree-Based Models**: Decision Trees, Random Forests, Gradient Boosting.
  - **Reason**: These models split data at thresholds, making them insensitive to feature magnitudes.

---

### Methods of Feature Scaling

#### 1. **Min-Max Scaling (Normalization)**
- **Definition**:
  - Rescales features to a fixed range, usually [0, 1].
  - Formula:
    \[
    x' = \frac{x - \text{min}(x)}{\text{max}(x) - \text{min}(x)}
    \]
- **When to Use**:
  - When the model is sensitive to the magnitude of feature values (e.g., distance-based models).
- **Pros**:
  - Preserves the distribution of values within a fixed range.
- **Cons**:
  - Sensitive to outliers, as they can distort the scaling.

---

#### 2. **Standardization (Z-Score Scaling)**
- **Definition**:
  - Centers features by subtracting the mean and scaling to unit variance.
  - Formula:
    \[
    x' = \frac{x - \mu}{\sigma}
    \]
    where \( \mu \) is the mean, and \( \sigma \) is the standard deviation.
- **When to Use**:
  - For models that assume normally distributed data or gradient-based algorithms.
- **Pros**:
  - Handles outliers better than Min-Max scaling.
  - Centers data around 0.
- **Cons**:
  - Does not bound the feature range.

---

#### 3. **Robust Scaling**
- **Definition**:
  - Centers and scales features using the median and interquartile range (IQR).
  - Formula:
    \[
    x' = \frac{x - \text{median}(x)}{\text{IQR}(x)}
    \]
- **When to Use**:
  - For datasets with outliers, as it is robust to extreme values.
- **Pros**:
  - Reduces the influence of outliers.
- **Cons**:
  - May not preserve the original data distribution.

---

#### 4. **MaxAbs Scaling**
- **Definition**:
  - Scales features by dividing each value by the maximum absolute value in the feature.
  - Formula:
    \[
    x' = \frac{x}{\text{max}(|x|)}
    \]
- **When to Use**:
  - When data contains only positive or negative values.
- **Pros**:
  - Maintains sparsity (important for sparse data).
- **Cons**:
  - Sensitive to outliers.

---

#### 5. **Logarithmic Scaling**
- **Definition**:
  - Applies a logarithmic transformation to compress the range of values.
  - Formula:
    \[
    x' = \log(x + 1)
    \]
- **When to Use**:
  - For skewed data or features with exponential growth.
- **Pros**:
  - Reduces the effect of large values.
  - Helps normalize skewed distributions.
- **Cons**:
  - Only applicable to positive values.

---

### Summary Table

| **Method**          | **Formula**                                       | **Best For**                              | **Pros**                            | **Cons**                           |
|----------------------|---------------------------------------------------|-------------------------------------------|-------------------------------------|-------------------------------------|
| **Min-Max Scaling**  | \( x' = \frac{x - \text{min}(x)}{\text{max}(x)} \) | Distance-based models (e.g., KNN)         | Simple, preserves range             | Sensitive to outliers              |
| **Standardization**  | \( x' = \frac{x - \mu}{\sigma} \)                 | Gradient-based models (e.g., regression)  | Centers data, handles outliers      | Does not bound range               |
| **Robust Scaling**   | \( x' = \frac{x - \text{median}(x)}{\text{IQR}(x)} \) | Data with outliers                        | Robust to outliers                  | May distort small ranges           |
| **MaxAbs Scaling**   | \( x' = \frac{x}{\text{max}(|x|)} \)              | Sparse datasets                           | Maintains sparsity                  | Sensitive to outliers              |
| **Log Scaling**      | \( x' = \log(x + 1) \)                            | Skewed data                               | Reduces skewness                    | Only for positive values           |

---

### Practical Tips
1. **Understand Your Model**:
   - Identify whether the algorithm is sensitive to feature magnitudes or distributions.
2. **Handle Outliers First**:
   - Consider robust scaling methods or preprocess outliers before scaling.
3. **Test Different Methods**:
   - Use cross-validation to evaluate the impact of scaling on model performance.
4. **Standardize for Normality**:
   - Standardization is preferred for normally distributed features.

Feature scaling is a crucial preprocessing step that ensures fair weighting, faster convergence, and optimal performance in many machine learning algorithms.

<a id='standardisation'></a>
# STANDARDISATION
### Standardization: Detailed Overview

#### What is Standardization?
Standardization is a feature scaling method that transforms data to have a **mean of 0** and a **standard deviation of 1**. It ensures that features contribute equally to the model by normalizing their scale.

---

### Standardization Formula

For a feature \( x \):
\[
x' = \frac{x - \mu}{\sigma}
\]
Where:
- \( \mu \): Mean of the feature (\( \mu = \frac{1}{n} \sum_{i=1}^n x_i \)).
- \( \sigma \): Standard deviation of the feature (\( \sigma = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2} \)).
- \( x' \): Standardized value of \( x \).

---

### Why Use Standardization?

1. **Equal Contribution**:
   - Ensures all features have the same scale, preventing features with larger magnitudes from dominating the model.
   
2. **Improves Convergence**:
   - Gradient-based optimization algorithms (e.g., gradient descent) converge faster when features are standardized, as it avoids skewed or flat gradients.

3. **Required by Certain Models**:
   - Algorithms like Principal Component Analysis (PCA), Support Vector Machines (SVM), and Logistic Regression assume standardized data for optimal performance.

4. **Handles Outliers Better than Min-Max Scaling**:
   - Since it is based on the mean and standard deviation, it is less sensitive to individual extreme values compared to range-based methods.

---

### When to Use Standardization?

- **Models Sensitive to Feature Scale**:
  - Linear models (Linear/Logistic Regression).
  - Distance-based models (KNN, K-Means).
  - Gradient-based models (Neural Networks, SVM).
  
- **Features with Different Units**:
  - When features have varying magnitudes or units (e.g., age in years, income in thousands).

- **Data Following Normal Distribution**:
  - Standardization works well when the data is approximately normally distributed, as it centers and scales the distribution.

---

### Impact of Standardization on Data

1. **Mean**:
   - After standardization, the mean of the transformed feature is \( 0 \):
     \[
     \frac{\sum (x - \mu)}{n} = 0
     \]

2. **Variance**:
   - The variance of the standardized feature becomes \( 1 \):
     \[
     \text{Var}(x') = \frac{\text{Var}(x)}{\sigma^2} = 1
     \]

3. **Distribution**:
   - The shape of the distribution remains unchanged. Standardization does **not make the data normal**, but it scales it to a unit distribution.

---

### Advantages of Standardization

| **Advantage**                        | **Explanation**                                                         |
|--------------------------------------|-------------------------------------------------------------------------|
| **Equalizes Feature Contribution**   | Ensures no single feature dominates due to magnitude differences.       |
| **Improves Optimization**            | Speeds up gradient-based algorithms by providing consistent gradients.  |
| **Centers Data**                     | Centers features around 0, making patterns more apparent.               |
| **Handles Large Ranges**             | Reduces the impact of features with large value ranges.                 |

---

### Disadvantages of Standardization

| **Disadvantage**                     | **Explanation**                                                         |
|--------------------------------------|-------------------------------------------------------------------------|
| **Not Robust to Outliers**           | Outliers can distort the mean and standard deviation, affecting scaling.|
| **Does Not Bound Values**            | Unlike Min-Max scaling, values are not confined to a specific range.    |
| **Requires Pre-Calculation**         | Mean and standard deviation must be computed for each feature.          |
| **Assumes Normality**                | Performs best with approximately normally distributed data.             |

---

### Practical Example of Standardization

#### Original Data:
| Feature | Value |
|---------|-------|
| \( x_1 \) | 100   |
| \( x_2 \) | 150   |
| \( x_3 \) | 200   |
| \( x_4 \) | 250   |

#### Step 1: Calculate the Mean and Standard Deviation
- \( \mu = \frac{100 + 150 + 200 + 250}{4} = 175 \)
- \( \sigma = \sqrt{\frac{(100 - 175)^2 + (150 - 175)^2 + (200 - 175)^2 + (250 - 175)^2}{4}} = 55.9 \)

#### Step 2: Standardize Each Value
\[
x'_i = \frac{x_i - \mu}{\sigma}
\]
- \( x_1' = \frac{100 - 175}{55.9} = -1.34 \)
- \( x_2' = \frac{150 - 175}{55.9} = -0.45 \)
- \( x_3' = \frac{200 - 175}{55.9} = 0.45 \)
- \( x_4' = \frac{250 - 175}{55.9} = 1.34 \)

#### Standardized Data:
| Feature | Standardized Value |
|---------|--------------------|
| \( x_1 \) | -1.34             |
| \( x_2 \) | -0.45             |
| \( x_3 \) | 0.45              |
| \( x_4 \) | 1.34              |

---

### Comparison with Other Scaling Methods

| **Aspect**               | **Standardization**                          | **Min-Max Scaling**                         | **Robust Scaling**                         |
|--------------------------|----------------------------------------------|---------------------------------------------|--------------------------------------------|
| **Output Range**         | Unbounded                                   | [0, 1]                                     | Scaled by IQR                              |
| **Handles Outliers**     | Sensitive                                   | Very sensitive                             | Robust                                     |
| **Centers Data**         | Yes (around 0)                              | No                                         | Yes (around median)                        |
| **Best Use Case**        | Models assuming normality or requiring consistent scales | Distance or gradient-based models         | Data with significant outliers             |

---

### Best Practices for Standardization

1. **Preprocess Outliers**:
   - Handle outliers before standardizing to avoid distortion.
   - Consider capping, transforming, or removing extreme values.

2. **Use Separate Scaling for Training and Test Sets**:
   - Fit the mean and standard deviation on the training set only.
   - Apply the same scaling parameters to the test set.

3. **Pair with Regularization**:
   - Standardization works well with regularized models (e.g., Ridge, Lasso).

4. **Evaluate Impact**:
   - Validate the effect of standardization on model performance using cross-validation.

---

### Key Takeaways
- Standardization is a powerful scaling method that centers data around 0 and scales it to unit variance.
- It is essential for gradient-based and distance-based models.
- While it handles large ranges well, it is sensitive to outliers and does not bound feature values.

<a id='min-max-scaling'></a>
# MIN MAX SCALING
### Min-Max Scaling: Detailed Overview

#### What is Min-Max Scaling?
Min-Max Scaling (also known as normalization) transforms feature values to a specific range, typically [0, 1]. This method rescales features so that the minimum value of the feature becomes 0, and the maximum value becomes 1.

---

### Min-Max Scaling Formula

For a feature \( x \), the scaled value \( x' \) is calculated as:
\[
x' = \frac{x - \text{min}(x)}{\text{max}(x) - \text{min}(x)}
\]
Where:
- \( \text{min}(x) \): Minimum value of the feature.
- \( \text{max}(x) \): Maximum value of the feature.
- \( x' \): Scaled value of \( x \).

If a custom range [\( a, b \)] is desired, the formula becomes:
\[
x' = a + \left( \frac{(x - \text{min}(x))}{\text{max}(x) - \text{min}(x)} \right) \cdot (b - a)
\]

---

### Why Use Min-Max Scaling?

1. **Brings Features to a Uniform Scale**:
   - Ensures all features have values in the same range, making them comparable.

2. **Prevents Dominance by Large-Scale Features**:
   - Features with large magnitudes can dominate the model if not scaled.

3. **Improves Convergence**:
   - Gradient-based optimization algorithms converge faster when feature values are within a small, uniform range.

4. **Prepares Data for Models Sensitive to Scales**:
   - Necessary for models relying on distance metrics (e.g., KNN, K-Means, SVM).

---

### When to Use Min-Max Scaling?

- **Distance-Based Models**:
  - K-Nearest Neighbors (KNN), K-Means, Principal Component Analysis (PCA).
- **Gradient-Based Models**:
  - Neural Networks, Logistic Regression, Linear Regression.
- **Features with Known Boundaries**:
  - When features naturally fit into a fixed range, Min-Max Scaling ensures consistency.

---

### Impact of Min-Max Scaling on Data

1. **Range**:
   - After scaling, all feature values lie within the specified range, typically [0, 1].
2. **Proportional Relationships**:
   - The relative distances between feature values are preserved.

---

### Advantages of Min-Max Scaling

| **Advantage**                        | **Description**                                                         |
|--------------------------------------|-------------------------------------------------------------------------|
| **Simple and Intuitive**             | Easy to implement and understand.                                       |
| **Preserves Relationships**          | Maintains the relative distances between feature values.                |
| **Prepares Data for Scale-Sensitive Models** | Essential for distance- or gradient-based algorithms.                   |

---

### Disadvantages of Min-Max Scaling

| **Disadvantage**                     | **Description**                                                         |
|--------------------------------------|-------------------------------------------------------------------------|
| **Sensitive to Outliers**            | Outliers can skew the scaling range, compressing most data into a small range.|
| **Assumes Fixed Range**              | New data points outside the original range may result in unexpected scaled values. |
| **Does Not Center Data**             | Unlike standardization, Min-Max scaling does not center data around 0. |

---

### Practical Example of Min-Max Scaling

#### Original Data:
| Feature | Value |
|---------|-------|
| \( x_1 \) | 100   |
| \( x_2 \) | 150   |
| \( x_3 \) | 200   |
| \( x_4 \) | 250   |

#### Step 1: Calculate the Minimum and Maximum
- \( \text{min}(x) = 100 \)
- \( \text{max}(x) = 250 \)

#### Step 2: Apply the Scaling Formula
\[
x' = \frac{x - \text{min}(x)}{\text{max}(x) - \text{min}(x)}
\]
- \( x_1' = \frac{100 - 100}{250 - 100} = 0.0 \)
- \( x_2' = \frac{150 - 100}{250 - 100} = 0.33 \)
- \( x_3' = \frac{200 - 100}{250 - 100} = 0.67 \)
- \( x_4' = \frac{250 - 100}{250 - 100} = 1.0 \)

#### Scaled Data:
| Feature | Scaled Value |
|---------|--------------|
| \( x_1 \) | 0.0          |
| \( x_2 \) | 0.33         |
| \( x_3 \) | 0.67         |
| \( x_4 \) | 1.0          |

---

### Comparison with Other Scaling Methods

| **Aspect**              | **Min-Max Scaling**                      | **Standardization**                  | **Robust Scaling**                  |
|--------------------------|------------------------------------------|---------------------------------------|--------------------------------------|
| **Output Range**         | [0, 1] (or custom range)                | Unbounded                           | Scaled by IQR                       |
| **Handles Outliers**     | Poorly                                  | Moderately                          | Well                                |
| **Centers Data**         | No                                      | Yes                                  | Centers around median               |
| **Best Use Case**        | Distance- or scale-sensitive models     | Gradient-based models                | Data with significant outliers      |

---

### Best Practices for Min-Max Scaling

1. **Handle Outliers First**:
   - Preprocess outliers (e.g., capping or removing) before applying Min-Max Scaling.

2. **Apply to Training Data**:
   - Compute \( \text{min}(x) \) and \( \text{max}(x) \) on the training data only. Use these values to scale the test and validation datasets.

3. **Use for Bounded Features**:
   - Best suited for features that naturally fit into a specific range (e.g., percentages, normalized scores).

4. **Avoid When Outliers Dominate**:
   - If outliers significantly skew the range, consider alternative scaling methods like robust scaling.

---

### Key Takeaways
- Min-Max Scaling rescales features to a fixed range, typically [0, 1], ensuring uniformity across variables.
- It is crucial for models that rely on distances or gradients.
- However, it is sensitive to outliers and may not be ideal for datasets with extreme values.

<a id='mean'></a>
# MEAN NORMALIZATION
### Mean Normalization: Detailed Overview

#### What is Mean Normalization?
Mean normalization is a feature scaling technique that transforms features to have a **mean of 0** and typically scales the feature values to lie within the range \([-1, 1]\). It ensures that the feature values are centered around zero and compressed into a bounded range.

---

### Mean Normalization Formula

For a feature \( x \), the normalized value \( x' \) is calculated as:
\[
x' = \frac{x - \mu}{\text{max}(x) - \text{min}(x)}
\]
Where:
- \( \mu \): Mean of the feature (\( \mu = \frac{1}{n} \sum_{i=1}^n x_i \)).
- \( \text{max}(x) \): Maximum value of the feature.
- \( \text{min}(x) \): Minimum value of the feature.

---

### Why Use Mean Normalization?

1. **Centers Data**:
   - Shifts the mean of each feature to 0, which makes patterns more apparent and helps models perform better.

2. **Scales Features**:
   - Compresses feature values to a fixed range (often \([-1, 1]\)), ensuring uniform contribution across variables.

3. **Improves Gradient-Based Algorithms**:
   - Gradient-based models (e.g., Logistic Regression, Neural Networks) converge faster with normalized data as it avoids steep or flat gradients.

4. **Handles Features with Different Units**:
   - Useful when features have different magnitudes or measurement units (e.g., age in years, salary in thousands).

---

### When to Use Mean Normalization?

- **Gradient-Based Models**:
  - Linear Regression, Logistic Regression, and Neural Networks benefit significantly from mean normalization.
- **Distance-Based Models**:
  - K-Nearest Neighbors (KNN), K-Means Clustering.
- **Features with Varying Scales**:
  - When feature ranges differ significantly, normalization ensures fair weighting.

---

### Impact of Mean Normalization on Data

1. **Mean**:
   - After normalization, the mean of the transformed feature becomes 0:
     \[
     \frac{\sum (x - \mu)}{n} = 0
     \]

2. **Range**:
   - The normalized feature values typically lie between \([-1, 1]\), ensuring uniformity across features.

---

### Advantages of Mean Normalization

| **Advantage**                        | **Description**                                                         |
|--------------------------------------|-------------------------------------------------------------------------|
| **Centers Data**                     | Shifts feature mean to 0, making patterns easier to analyze.            |
| **Improves Gradient Descent**        | Speeds up convergence by avoiding steep or flat gradients.              |
| **Uniform Contribution**             | Scales features to a similar range, ensuring equal importance.          |
| **Handles Different Units**          | Effectively deals with features measured on different scales.           |

---

### Disadvantages of Mean Normalization

| **Disadvantage**                     | **Description**                                                         |
|--------------------------------------|-------------------------------------------------------------------------|
| **Sensitive to Outliers**            | Outliers can skew the mean, distorting normalization.                   |
| **Assumes Linear Relationships**     | Works best with features that have approximately linear relationships.  |
| **Range May Vary**                   | If not carefully implemented, the range may not exactly fall in \([-1, 1]\).|

---

### Practical Example of Mean Normalization

#### Original Data:
| Feature | Value |
|---------|-------|
| \( x_1 \) | 100   |
| \( x_2 \) | 150   |
| \( x_3 \) | 200   |
| \( x_4 \) | 250   |

#### Step 1: Calculate the Mean, Maximum, and Minimum
- \( \mu = \frac{100 + 150 + 200 + 250}{4} = 175 \)
- \( \text{max}(x) = 250 \)
- \( \text{min}(x) = 100 \)

#### Step 2: Apply the Normalization Formula
\[
x' = \frac{x - \mu}{\text{max}(x) - \text{min}(x)}
\]
- \( x_1' = \frac{100 - 175}{250 - 100} = \frac{-75}{150} = -0.5 \)
- \( x_2' = \frac{150 - 175}{250 - 100} = \frac{-25}{150} = -0.17 \)
- \( x_3' = \frac{200 - 175}{250 - 100} = \frac{25}{150} = 0.17 \)
- \( x_4' = \frac{250 - 175}{250 - 100} = \frac{75}{150} = 0.5 \)

#### Normalized Data:
| Feature | Normalized Value |
|---------|------------------|
| \( x_1 \) | -0.5            |
| \( x_2 \) | -0.17           |
| \( x_3 \) | 0.17            |
| \( x_4 \) | 0.5             |

---

### Comparison with Other Scaling Methods

| **Aspect**              | **Mean Normalization**                 | **Min-Max Scaling**                         | **Standardization**                 |
|--------------------------|----------------------------------------|---------------------------------------------|-------------------------------------|
| **Centers Data**         | Yes (mean = 0)                        | No                                          | Yes (mean = 0)                     |
| **Output Range**         | Typically \([-1, 1]\)                 | Typically [0, 1]                            | Unbounded                          |
| **Handles Outliers**     | Poorly                                | Poorly                                      | Moderately                         |
| **Best Use Case**        | Gradient- or distance-based models    | Features with fixed boundaries              | Normal distribution assumptions    |

---

### Best Practices for Mean Normalization

1. **Handle Outliers First**:
   - Preprocess or remove outliers to avoid distorting the mean.
2. **Fit on Training Data Only**:
   - Compute the mean, max, and min values from the training data and apply them to test data for consistency.
3. **Combine with Regularization**:
   - Pair normalization with regularized models (e.g., Ridge, Lasso) to prevent overfitting.
4. **Validate Performance**:
   - Evaluate the impact of normalization on model performance using cross-validation.

---

### Key Takeaways
- Mean normalization centers features around 0 and rescales them to a fixed range, typically \([-1, 1]\).
- It is most effective for models sensitive to feature scales, such as gradient- or distance-based algorithms.
- While simple and intuitive, mean normalization is sensitive to outliers and should be used with preprocessing when extreme values are present.

<a id='max-abs'></a>
# MAX ABS SACLING
### Mean Normalization: Detailed Overview

#### What is Mean Normalization?
Mean normalization is a feature scaling technique that transforms features to have a **mean of 0** and typically scales the feature values to lie within the range \([-1, 1]\). It ensures that the feature values are centered around zero and compressed into a bounded range.

---

### Mean Normalization Formula

For a feature \( x \), the normalized value \( x' \) is calculated as:
\[
x' = \frac{x - \mu}{\text{max}(x) - \text{min}(x)}
\]
Where:
- \( \mu \): Mean of the feature (\( \mu = \frac{1}{n} \sum_{i=1}^n x_i \)).
- \( \text{max}(x) \): Maximum value of the feature.
- \( \text{min}(x) \): Minimum value of the feature.

---

### Why Use Mean Normalization?

1. **Centers Data**:
   - Shifts the mean of each feature to 0, which makes patterns more apparent and helps models perform better.

2. **Scales Features**:
   - Compresses feature values to a fixed range (often \([-1, 1]\)), ensuring uniform contribution across variables.

3. **Improves Gradient-Based Algorithms**:
   - Gradient-based models (e.g., Logistic Regression, Neural Networks) converge faster with normalized data as it avoids steep or flat gradients.

4. **Handles Features with Different Units**:
   - Useful when features have different magnitudes or measurement units (e.g., age in years, salary in thousands).

---

### When to Use Mean Normalization?

- **Gradient-Based Models**:
  - Linear Regression, Logistic Regression, and Neural Networks benefit significantly from mean normalization.
- **Distance-Based Models**:
  - K-Nearest Neighbors (KNN), K-Means Clustering.
- **Features with Varying Scales**:
  - When feature ranges differ significantly, normalization ensures fair weighting.

---

### Impact of Mean Normalization on Data

1. **Mean**:
   - After normalization, the mean of the transformed feature becomes 0:
     \[
     \frac{\sum (x - \mu)}{n} = 0
     \]

2. **Range**:
   - The normalized feature values typically lie between \([-1, 1]\), ensuring uniformity across features.

---

### Advantages of Mean Normalization

| **Advantage**                        | **Description**                                                         |
|--------------------------------------|-------------------------------------------------------------------------|
| **Centers Data**                     | Shifts feature mean to 0, making patterns easier to analyze.            |
| **Improves Gradient Descent**        | Speeds up convergence by avoiding steep or flat gradients.              |
| **Uniform Contribution**             | Scales features to a similar range, ensuring equal importance.          |
| **Handles Different Units**          | Effectively deals with features measured on different scales.           |

---

### Disadvantages of Mean Normalization

| **Disadvantage**                     | **Description**                                                         |
|--------------------------------------|-------------------------------------------------------------------------|
| **Sensitive to Outliers**            | Outliers can skew the mean, distorting normalization.                   |
| **Assumes Linear Relationships**     | Works best with features that have approximately linear relationships.  |
| **Range May Vary**                   | If not carefully implemented, the range may not exactly fall in \([-1, 1]\).|

---

### Practical Example of Mean Normalization

#### Original Data:
| Feature | Value |
|---------|-------|
| \( x_1 \) | 100   |
| \( x_2 \) | 150   |
| \( x_3 \) | 200   |
| \( x_4 \) | 250   |

#### Step 1: Calculate the Mean, Maximum, and Minimum
- \( \mu = \frac{100 + 150 + 200 + 250}{4} = 175 \)
- \( \text{max}(x) = 250 \)
- \( \text{min}(x) = 100 \)

#### Step 2: Apply the Normalization Formula
\[
x' = \frac{x - \mu}{\text{max}(x) - \text{min}(x)}
\]
- \( x_1' = \frac{100 - 175}{250 - 100} = \frac{-75}{150} = -0.5 \)
- \( x_2' = \frac{150 - 175}{250 - 100} = \frac{-25}{150} = -0.17 \)
- \( x_3' = \frac{200 - 175}{250 - 100} = \frac{25}{150} = 0.17 \)
- \( x_4' = \frac{250 - 175}{250 - 100} = \frac{75}{150} = 0.5 \)

#### Normalized Data:
| Feature | Normalized Value |
|---------|------------------|
| \( x_1 \) | -0.5            |
| \( x_2 \) | -0.17           |
| \( x_3 \) | 0.17            |
| \( x_4 \) | 0.5             |

---

### Comparison with Other Scaling Methods

| **Aspect**              | **Mean Normalization**                 | **Min-Max Scaling**                         | **Standardization**                 |
|--------------------------|----------------------------------------|---------------------------------------------|-------------------------------------|
| **Centers Data**         | Yes (mean = 0)                        | No                                          | Yes (mean = 0)                     |
| **Output Range**         | Typically \([-1, 1]\)                 | Typically [0, 1]                            | Unbounded                          |
| **Handles Outliers**     | Poorly                                | Poorly                                      | Moderately                         |
| **Best Use Case**        | Gradient- or distance-based models    | Features with fixed boundaries              | Normal distribution assumptions    |

---

### Best Practices for Mean Normalization

1. **Handle Outliers First**:
   - Preprocess or remove outliers to avoid distorting the mean.
2. **Fit on Training Data Only**:
   - Compute the mean, max, and min values from the training data and apply them to test data for consistency.
3. **Combine with Regularization**:
   - Pair normalization with regularized models (e.g., Ridge, Lasso) to prevent overfitting.
4. **Validate Performance**:
   - Evaluate the impact of normalization on model performance using cross-validation.

---

### Key Takeaways
- Mean normalization centers features around 0 and rescales them to a fixed range, typically \([-1, 1]\).
- It is most effective for models sensitive to feature scales, such as gradient- or distance-based algorithms.
- While simple and intuitive, mean normalization is sensitive to outliers and should be used with preprocessing when extreme values are present.

<a id='robust'></a>
# ROBUST
### Robust Scaling: Detailed Overview

#### What is Robust Scaling?
Robust scaling is a feature scaling method that uses the **median** and the **interquartile range (IQR)** to transform features. It is designed to reduce the impact of **outliers** by relying on robust statistics that are less sensitive to extreme values compared to the mean and standard deviation.

---

### Robust Scaling Formula

For a feature \( x \), the scaled value \( x' \) is computed as:
\[
x' = \frac{x - \text{median}(x)}{\text{IQR}(x)}
\]
Where:
- \( \text{median}(x) \): The median of the feature.
- \( \text{IQR}(x) = Q3 - Q1 \): The interquartile range, where:
  - \( Q1 \): The first quartile (25th percentile).
  - \( Q3 \): The third quartile (75th percentile).

Optionally, a custom scaling range (e.g., \([-1, 1]\)) can be applied after transformation.

---

### Why Use Robust Scaling?

1. **Handles Outliers**:
   - By focusing on the median and IQR, robust scaling minimizes the influence of extreme values.

2. **Centers Data**:
   - Centers features around the median, making patterns more interpretable.

3. **Scales Robustly**:
   - Scales data relative to the spread of the middle 50% of values, ensuring robust transformation.

---

### When to Use Robust Scaling?

- **Data with Outliers**:
  - Ideal for datasets containing extreme values that would skew traditional scaling methods (e.g., Min-Max Scaling, Standardization).

- **Features with Skewed Distributions**:
  - Works well for variables that are not normally distributed.

- **Models Sensitive to Scale**:
  - Suitable for gradient- and distance-based models, such as Logistic Regression, SVM, and KNN.

---

### Advantages of Robust Scaling

| **Advantage**                        | **Description**                                                         |
|--------------------------------------|-------------------------------------------------------------------------|
| **Handles Outliers Effectively**     | Minimizes the impact of extreme values by using median and IQR.         |
| **Centers Data**                     | Centers features around the median, improving interpretability.         |
| **Scales Robustly**                  | Ensures consistent scaling even with skewed data.                       |

---

### Disadvantages of Robust Scaling

| **Disadvantage**                     | **Description**                                                         |
|--------------------------------------|-------------------------------------------------------------------------|
| **Ignores Outlier Information**      | Outliers are downplayed, which might be valuable in some analyses.       |
| **Does Not Normalize the Range**     | The resulting range of the scaled features is not fixed (unlike Min-Max scaling). |
| **Limited Effect on Normally Distributed Data** | May not provide significant benefits for data with few or no outliers.   |

---

### Practical Example of Robust Scaling

#### Original Data:
| Feature | Value |
|---------|-------|
| \( x_1 \) | 10    |
| \( x_2 \) | 50    |
| \( x_3 \) | 100   |
| \( x_4 \) | 2000  |  *(Outlier)*

#### Step 1: Calculate the Median and IQR
- **Median**:
  \[
  \text{median}(x) = \text{50th percentile} = 75
  \]
- **IQR**:
  \[
  \text{IQR}(x) = Q3 - Q1 = \text{(75th percentile)} - \text{(25th percentile)} = 100 - 30 = 70
  \]

#### Step 2: Apply the Scaling Formula
\[
x' = \frac{x - \text{median}(x)}{\text{IQR}(x)}
\]
- \( x_1' = \frac{10 - 75}{70} = -0.93 \)
- \( x_2' = \frac{50 - 75}{70} = -0.36 \)
- \( x_3' = \frac{100 - 75}{70} = 0.36 \)
- \( x_4' = \frac{2000 - 75}{70} = 27.5 \) *(Outlier remains large but less influential)*

#### Scaled Data:
| Feature | Scaled Value |
|---------|--------------|
| \( x_1 \) | -0.93       |
| \( x_2 \) | -0.36       |
| \( x_3 \) | 0.36        |
| \( x_4 \) | 27.5        |

---

### Comparison with Other Scaling Methods

| **Aspect**              | **Robust Scaling**                     | **Min-Max Scaling**                     | **Standardization**                 |
|--------------------------|----------------------------------------|-----------------------------------------|-------------------------------------|
| **Output Range**         | Not fixed                             | [0, 1]                                  | Unbounded                          |
| **Centers Data**         | Yes (around median)                   | No                                      | Yes (around mean)                  |
| **Handles Outliers**     | Effectively                           | Poorly                                  | Moderately                         |
| **Best Use Case**        | Data with significant outliers        | Features with fixed boundaries          | Normal distribution assumptions    |

---

### Best Practices for Robust Scaling

1. **Preprocess Data**:
   - Remove or cap extreme outliers before scaling, especially if they are errors or anomalies.

2. **Fit on Training Data Only**:
   - Compute the median and IQR on the training set and apply the same transformation to test and validation sets.

3. **Pair with Robust Models**:
   - Use robust scaling with models like Logistic Regression or SVM to improve performance in the presence of outliers.

4. **Validate Scaling**:
   - Use cross-validation to ensure that robust scaling improves model accuracy and stability.

---

### Key Takeaways
- **Definition**: Robust scaling uses the median and interquartile range to transform features, making it effective for datasets with outliers.
- **Best For**: Skewed or outlier-heavy data.
- **Advantages**: Handles outliers effectively, centers data around the median, and scales consistently.
- **Disadvantages**: Ignores outlier information and may not normalize the range.

Robust scaling is a practical and reliable scaling method for datasets where outliers would otherwise distort the results.

<a id='unit-norm'></a>
# UNIT NORM
### Unit Norm Scaling: Detailed Overview

#### What is Unit Norm Scaling?
Unit Norm Scaling transforms a feature vector so that its **norm** (magnitude) equals 1. This is done by dividing each feature value by the norm of the vector. It is commonly used when the magnitude of the vector is irrelevant, but the direction matters, as in text data or signal processing.

---

### Types of Norms

1. **L1 Norm**:
   - The sum of the absolute values of the vector elements.
   - Formula:
     \[
     \|x\|_1 = \sum_{i=1}^n |x_i|
     \]
   - Scaled Value:
     \[
     x'_i = \frac{x_i}{\|x\|_1}
     \]

2. **L2 Norm (Euclidean Norm)**:
   - The square root of the sum of the squared elements.
   - Formula:
     \[
     \|x\|_2 = \sqrt{\sum_{i=1}^n x_i^2}
     \]
   - Scaled Value:
     \[
     x'_i = \frac{x_i}{\|x\|_2}
     \]

3. **Max Norm (Infinity Norm)**:
   - The maximum absolute value of the vector elements.
   - Formula:
     \[
     \|x\|_\infty = \max(|x_1|, |x_2|, \dots, |x_n|)
     \]
   - Scaled Value:
     \[
     x'_i = \frac{x_i}{\|x\|_\infty}
     \]

---

### Why Use Unit Norm Scaling?

1. **Ensures Equal Contribution**:
   - Forces all feature vectors to have the same magnitude, preventing features with larger values from dominating.

2. **Important for Direction-Based Analysis**:
   - When the orientation or direction of the vector is critical, as in cosine similarity or certain machine learning tasks.

3. **Standardization in Sparse Data**:
   - Particularly useful for sparse datasets (e.g., text data with TF-IDF features).

---

### When to Use Unit Norm Scaling?

- **Text Data**:
  - Normalizing word embeddings, TF-IDF values, or document vectors for tasks like text classification or clustering.

- **Similarity-Based Models**:
  - Essential for models using cosine similarity or dot product (e.g., recommendation systems, clustering).

- **Signal Processing**:
  - When analyzing signals or data where the magnitude of the vector is irrelevant.

---

### Advantages of Unit Norm Scaling

| **Advantage**                        | **Description**                                                         |
|--------------------------------------|-------------------------------------------------------------------------|
| **Removes Magnitude Influence**      | Ensures features contribute equally by focusing only on direction.      |
| **Useful for Cosine Similarity**     | Essential for applications relying on vector angles or orientation.     |
| **Preserves Sparsity**               | Zero entries remain zero, maintaining sparse data structure.            |

---

### Disadvantages of Unit Norm Scaling

| **Disadvantage**                     | **Description**                                                         |
|--------------------------------------|-------------------------------------------------------------------------|
| **Sensitive to Outliers**            | Outliers can distort the norm, affecting all scaled values.             |
| **Assumes Non-Zero Norm**            | Cannot handle vectors with a norm of 0 (e.g., all-zero vectors).         |
| **Range Not Fixed**                  | The transformed values are not confined to a specific range.            |

---

### Practical Example of Unit Norm Scaling

#### Original Data:
| Feature Vector | Value       |
|----------------|-------------|
| \( x_1 \)      | \( [3, 4] \) |
| \( x_2 \)      | \( [1, -1] \) |

#### Step 1: Calculate the L2 Norm
- \( \|x_1\|_2 = \sqrt{3^2 + 4^2} = \sqrt{9 + 16} = 5 \)
- \( \|x_2\|_2 = \sqrt{1^2 + (-1)^2} = \sqrt{1 + 1} = \sqrt{2} \)

#### Step 2: Scale the Values
- For \( x_1 \): \( x'_1 = \frac{[3, 4]}{5} = [0.6, 0.8] \)
- For \( x_2 \): \( x'_2 = \frac{[1, -1]}{\sqrt{2}} = [0.707, -0.707] \)

#### Scaled Data:
| Feature Vector | Scaled Values      |
|----------------|--------------------|
| \( x_1 \)      | \( [0.6, 0.8] \)   |
| \( x_2 \)      | \( [0.707, -0.707] \) |

---

### Comparison with Other Scaling Methods

| **Aspect**              | **Unit Norm Scaling**                  | **Standardization**                 | **Min-Max Scaling**                |
|--------------------------|----------------------------------------|-------------------------------------|-------------------------------------|
| **Focus**               | Ensures norm of 1                     | Centers data to mean 0              | Scales to a fixed range            |
| **Handles Sparsity**     | Yes                                   | No                                  | No                                 |
| **Handles Outliers**     | Poorly                                | Moderately                          | Poorly                             |
| **Best Use Case**        | Directional analysis, sparse data     | Normally distributed data           | Features with fixed boundaries     |

---

### Best Practices for Unit Norm Scaling

1. **Handle Zero Norms**:
   - Remove or treat zero-norm vectors separately to avoid computational errors.

2. **Choose the Right Norm**:
   - **L1 Norm**: For interpretability in sparse data (e.g., text data).
   - **L2 Norm**: For Euclidean-based distance metrics or similarity.
   - **Max Norm**: For scale-sensitive applications.

3. **Preprocess Outliers**:
   - Handle outliers before scaling to prevent distortion of the norm.

4. **Test Different Norms**:
   - Experiment with different norms (L1, L2, Max) to find the most effective transformation for the task.

---

### Key Takeaways
- **Definition**: Unit Norm Scaling transforms features to ensure their norm equals 1, focusing on direction rather than magnitude.
- **Best For**: Text data, similarity-based models, or sparse datasets.
- **Advantages**: Maintains sparsity, removes magnitude influence, and is simple to compute.
- **Disadvantages**: Sensitive to outliers and requires non-zero norms.

Unit Norm Scaling is a powerful technique for tasks where the direction of feature vectors matters more than their magnitude.

<a id='comparison'></a>
# COMPARISON

### Comparison of Feature Scaling Methods

| **Scaling Method**        | **Formula**                                         | **Output Range**         | **Centers Data** | **Handles Outliers** | **Best For**                                     | **When to Use**                                                                 |
|----------------------------|----------------------------------------------------|--------------------------|-------------------|-----------------------|-------------------------------------------------|---------------------------------------------------------------------------------|
| **Min-Max Scaling**        | \( x' = \frac{x - \text{min}(x)}{\text{max}(x) - \text{min}(x)} \) | [0, 1] (or custom)       | No                | Poorly                | Distance-based models (KNN, K-Means), image data | When features have fixed boundaries or are bounded by nature.                    |
| **Standardization**        | \( x' = \frac{x - \mu}{\sigma} \)                  | Unbounded                | Yes (mean = 0)    | Moderately            | Gradient-based models (Linear Regression, SVM) | When data is normally distributed or for algorithms requiring zero-centered data. |
| **Robust Scaling**         | \( x' = \frac{x - \text{median}(x)}{\text{IQR}(x)} \) | Not fixed                | Yes (median = 0)  | Well                  | Data with significant outliers                  | When data contains outliers that would distort other scaling methods.            |
| **Max-Abs Scaling**        | \( x' = \frac{x}{\text{max}(|x|)} \)               | [-1, 1]                  | No                | Poorly                | Sparse datasets, distance-based models          | When preserving sparsity is important, e.g., TF-IDF, one-hot encoding.           |
| **Log Scaling**            | \( x' = \log(x + 1) \)                             | Compressed range         | No                | Moderately            | Skewed data                                    | When features have exponential growth or are highly skewed.                       |
| **Mean Normalization**     | \( x' = \frac{x - \mu}{\text{max}(x) - \text{min}(x)} \) | [-1, 1]                  | Yes (mean = 0)    | Poorly                | Gradient-based models                           | When centering data and scaling to a bounded range are both required.             |
| **Unit Norm Scaling**      | \( x'_i = \frac{x_i}{\|x\|_p} \) (e.g., L1, L2, Max norm) | Depends on norm          | No                | Poorly                | Text data (TF-IDF, embeddings), cosine similarity | When the magnitude is irrelevant, and the direction of the vector matters.        |

---

### Guidelines for Choosing a Scaling Method

#### 1. **When to Use Min-Max Scaling**
- Features have natural boundaries (e.g., percentages, pixel intensities).
- Models are distance-based (e.g., KNN, K-Means).
- **Avoid** if outliers dominate the feature range.

#### 2. **When to Use Standardization**
- Features follow approximately normal distributions.
- Models rely on gradient descent (e.g., Logistic Regression, Neural Networks).
- Useful for algorithms assuming data is zero-centered with unit variance.

#### 3. **When to Use Robust Scaling**
- Data contains significant outliers or skewness.
- When medians and IQR better represent the dataset than mean and standard deviation.
- Works well for distance- and gradient-based models.

#### 4. **When to Use Max-Abs Scaling**
- Data is sparse, and preserving sparsity is essential.
- Features contain both positive and negative values.
- **Ideal for** sparse datasets like text data (bag-of-words, TF-IDF).

#### 5. **When to Use Log Scaling**
- Features have exponential growth or are highly skewed.
- Data contains large positive values that need compression.
- **Avoid** if features contain zeros or negative values.

#### 6. **When to Use Mean Normalization**
- Data must be centered around zero and scaled to a bounded range (e.g., [-1, 1]).
- When relative differences are more important than absolute values.

#### 7. **When to Use Unit Norm Scaling**
- When only the direction of the feature vector matters (e.g., text embeddings, cosine similarity).
- Sparse datasets or data used in distance-based models like clustering or recommendation systems.

---

### Summary Table of Use Cases

| **Scaling Method**        | **Best Use Cases**                                                                                                                                                     |
|----------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Min-Max Scaling**        | Image data, bounded features, distance-based models (KNN, K-Means).                                                                                                  |
| **Standardization**        | Normally distributed data, gradient-based models (Logistic Regression, SVM, Neural Networks).                                                                       |
| **Robust Scaling**         | Skewed data, datasets with significant outliers.                                                                                                                     |
| **Max-Abs Scaling**        | Sparse datasets (e.g., TF-IDF, one-hot encoding), distance-based models.                                                                                             |
| **Log Scaling**            | Exponential growth (e.g., financial data), highly skewed features.                                                                                                   |
| **Mean Normalization**     | Models requiring data centered at zero and scaled to a bounded range.                                                                                                |
| **Unit Norm Scaling**      | Text data (e.g., TF-IDF), embeddings, cosine similarity, clustering, and sparse datasets where direction matters more than magnitude.                                |

Choosing the appropriate scaling method depends on the data characteristics, the presence of outliers, and the algorithm's sensitivity to scale. Always validate the scaling method's impact on model performance using cross-validation.
