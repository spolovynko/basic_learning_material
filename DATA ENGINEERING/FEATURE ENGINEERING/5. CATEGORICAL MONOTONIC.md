- [MONOTONIC](#monotonic)
- [ORDER LABEL ENCODING](#order-label-encoding)
- [MEAN ENCODING](#mean-encoding)
- [WEIGHT OF EVIDENCE](#weight-of-evidence)

<a id='monotonic'></a>
# Monotonic

Monotonic categorical encoding is a technique to convert categorical variables into numerical representations while preserving a **monotonic relationship** with the target variable. This is particularly useful when the relationship between the categorical feature and the target is ordered or exhibits a consistent trend.

---

## Key Concepts in Monotonic Categorical Encoding

### 1. Monotonic Relationship
- A **monotonic relationship** exists when changes in the input variable consistently lead to either increases or decreases in the target variable.
- **Example**: Higher customer income categories may lead to a higher likelihood of loan approval.

### 2. Order Label Encoding
- Assigns numeric values to categories based on their natural order or predefined logic.
- **Example**:
  - Categories: `Low`, `Medium`, `High`
  - Encoding: `Low = 1`, `Medium = 2`, `High = 3`
- **Use Case**: Suitable for variables with an inherent order, like `low`, `medium`, and `high`.

### 3. Mean Encoding
- Replaces each category with the mean of the target variable for that category.
- **Example**:
  - Categories: `A`, `B`, `C`
  - Target Means: `A = 0.8`, `B = 0.5`, `C = 0.3`
- **Drawback**: Prone to overfitting, especially with small datasets. Regularization can mitigate this.

### 4. Weight of Evidence (WoE)
- Used extensively in risk modeling (e.g., credit scoring) and ensures monotonicity by design.
- **Formula**:
  \[
  WoE = \ln \left( \frac{\text{Proportion of Good Outcomes}}{\text{Proportion of Bad Outcomes}} \right)
  \]
- **Example**: Categories with higher proportions of "good" outcomes relative to "bad" will have positive WoE values.
- **Advantages**:
  - Ensures monotonicity.
  - Reduces overfitting risks.
  - Highly interpretable.

---

## Implementation Steps for Monotonic Encoding

### 1. Analyze the Data
- Check for a monotonic relationship between the categorical feature and the target.
- Use visualization or correlation measures (e.g., Spearman's rank correlation).

### 2. Select Encoding Method
- **Order Label Encoding**: Use if categories have a natural order.
- **Mean Encoding or WoE**: Use if statistical relationships with the target are required.

### 3. Apply Regularization (if needed)
- Regularization reduces the impact of outliers in Mean Encoding or WoE.
- **Techniques**:
  - **Smoothing**: Combine the category mean with the overall target mean using a weight based on category size.
  - **Cross-validation**: Use out-of-fold target statistics to prevent overfitting.

### 4. Validate the Encoding
- Check for monotonicity and predictive power using model performance metrics.
- Use tools like **partial dependence plots (PDPs)** to verify alignment with expectations.

---

## Considerations and Best Practices

1. **Overfitting Risk**:
   - Mean Encoding and WoE may overfit if categories have few samples. Use regularization.

2. **Scalability**:
   - For high-cardinality categories, consider grouping or hashing before applying monotonic encoding.

3. **Interpretability**:
   - WoE is highly interpretable, making it favored in finance and risk modeling.

4. **Target Leakage**:
   - Avoid using target information from the test dataset. Use cross-validation techniques to encode categories without peeking.

---

## Example Use Case: Loan Default Prediction

### Problem
A dataset contains a categorical variable `Occupation` with categories like `Manager`, `Clerk`, and `Engineer`.

### Steps
1. Analyze the relationship between `Occupation` and loan default rate.
2. Use **WoE** to encode `Occupation` into numeric values.
3. Validate monotonicity using visualizations or correlation measures.
4. Incorporate the encoded variable into a machine learning model.

---

This approach ensures that encoded values maintain a meaningful and monotonic relationship with the target, improving both model interpretability and performance.

<a id='order-label-encoding'></a>
# ORDER LABEL ENCODING

### Definition
Order label encoding is a method for transforming categorical variables into numerical values based on a predefined or inherent order. Each category is assigned an integer that reflects its relative position in the order. This method is suitable for ordinal variables, where the categories have a meaningful sequence.

#### Example
- Categories: `Low`, `Medium`, `High`
- Encoding:
  - `Low = 1`
  - `Medium = 2`
  - `High = 3`

In this example, the order `Low < Medium < High` is preserved in the numerical representation.

---

### Advantages of Order Label Encoding

1. **Preserves Order Information**:
   - Maintains the natural or logical sequence of the categories, making it ideal for ordinal variables.

2. **Simple to Implement**:
   - Straightforward and computationally efficient.

3. **Improves Model Performance**:
   - Helps machine learning algorithms, particularly tree-based models, utilize the ordinal relationships effectively.

4. **Compact Representation**:
   - Reduces the complexity of the data by representing categories with integers instead of binary or one-hot vectors.

---

### Limitations of Order Label Encoding

1. **Assumes Equal Distances**:
   - Encoded values imply that the difference between categories is uniform, which may not reflect the actual relationship.
   - Example: The gap between `Low` and `Medium` may not be the same as between `Medium` and `High`.

2. **Not Suitable for Nominal Variables**:
   - Applying this encoding to unordered categories (e.g., colors, countries) introduces a false sense of order.

3. **Potential for Misinterpretation**:
   - Some machine learning models (e.g., linear regression) may interpret the numeric values as continuous and proportional, leading to incorrect assumptions.

4. **Limited Applicability**:
   - Effective only for ordinal features with clear and logical ordering.

5. **No Capture of Statistical Relationship**:
   - Does not incorporate information about the relationship between the feature and the target variable, which can reduce its effectiveness in predictive models.

---

### When to Use Order Label Encoding

- **Appropriate For**:
  - Ordinal variables with a clear, meaningful order.
  - Tree-based models (e.g., decision trees, random forests, XGBoost), as they handle categorical relationships naturally.

- **Avoid For**:
  - Nominal variables where there is no inherent order.
  - Models sensitive to numeric relationships (e.g., linear regression) when the ordinal distances are not uniform.

---

### Example Scenario

#### Dataset: Customer Satisfaction Levels
- Variable: `Satisfaction Level` with categories: `Dissatisfied`, `Neutral`, `Satisfied`
- Encoding:
  - `Dissatisfied = 1`
  - `Neutral = 2`
  - `Satisfied = 3`

**Pros**: Maintains the natural order of satisfaction levels.
**Cons**: Implies that the gap between `Dissatisfied` and `Neutral` is the same as between `Neutral` and `Satisfied`, which might not be true.

---

Order label encoding is a simple yet powerful method for ordinal variables, but it requires careful consideration of the feature's characteristics and the model's assumptions to avoid misrepresentation and misuse.

<a id='mean-encoding'></a>
# MEAN ENCODING

### Definition
Mean encoding (also known as target encoding) is a technique for transforming categorical variables into numerical representations by replacing each category with the **mean value of the target variable** for that category. This approach creates an encoding that reflects the statistical relationship between the feature and the target.

#### Formula
For a given category \( c \), the encoded value is:
\[
\text{Mean Encoding}(c) = \frac{\sum \text{Target Values for } c}{\text{Number of Instances of } c}
\]

#### Example
- Dataset:
  - `Category`: `A`, `B`, `C`
  - `Target`: `1`, `0`, `1`, `0`, `1`
- Mean encoding:
  - `A`: Target mean = \( \frac{2}{3} \approx 0.67 \)
  - `B`: Target mean = \( \frac{1}{2} = 0.5 \)
  - `C`: Target mean = \( \frac{1}{1} = 1.0 \)

The categories `A`, `B`, and `C` are replaced by their respective target means.

---

### Advantages of Mean Encoding

1. **Captures Statistical Relationship**:
   - Encodes information about how a category correlates with the target variable, making it highly effective in predictive modeling.

2. **Compact Representation**:
   - Reduces high-cardinality categorical features into a single numeric value for each category, saving memory and computational resources.

3. **Improves Model Performance**:
   - Particularly useful for tree-based models (e.g., random forests, XGBoost) and gradient-boosting frameworks, which benefit from the additional statistical information.

4. **Customizable via Regularization**:
   - Regularization methods, such as smoothing or cross-validation, can mitigate overfitting and improve generalization.

---

### Limitations of Mean Encoding

1. **Prone to Overfitting**:
   - In small datasets or for categories with few samples, mean encoding may overfit to noise in the data.
   - **Example**: A category with a single instance will have a mean identical to the target value for that instance, which can mislead the model.

2. **Target Leakage**:
   - If the mean encoding is computed using the target variable in the test data, it introduces **data leakage**, leading to over-optimistic performance metrics.

3. **Not Interpretable**:
   - The encoded values lack intuitive meaning compared to simpler encodings like one-hot or label encoding.

4. **Bias Towards Larger Categories**:
   - Categories with more samples can dominate the encoding unless proper regularization is applied.

5. **Dependent on Target Distribution**:
   - Heavily influenced by the target variable's distribution, which can distort relationships if the target is imbalanced.

---

### Techniques to Address Limitations

1. **Regularization with Smoothing**:
   - Combines the category-specific mean with the overall target mean, weighted by the size of the category:
     \[
     \text{Smoothing Encoding} = \frac{n \cdot \text{Category Mean} + k \cdot \text{Global Mean}}{n + k}
     \]
     - \( n \): Number of instances in the category.
     - \( k \): Smoothing factor to control the balance.

2. **Cross-Validation Encoding**:
   - Use out-of-fold means to encode categories, ensuring no leakage of target information into the test set.

3. **Noise Addition**:
   - Add random noise to encoded values to reduce overfitting.

---

### Example Scenario

#### Dataset: Product Popularity
- Variable: `Product Category`
- Target: `Product Sold (Yes=1, No=0)`

#### Steps:
1. Calculate the mean target value for each product category:
   - `Electronics`: 0.75 (75% of products sold)
   - `Clothing`: 0.40 (40% of products sold)
   - `Furniture`: 0.60 (60% of products sold)
2. Replace the categories with their respective mean values:
   - `Electronics = 0.75`, `Clothing = 0.40`, `Furniture = 0.60`.

---

### Advantages in Use Case
- Encodes the likelihood of sales based on historical data.
- Enhances model performance by reflecting category-specific trends.

### Limitations in Use Case
- **Small Categories**: If `Furniture` only had 5 sales, the encoding might overfit.
- **Data Leakage**: Improper splitting could lead to target leakage.

---

### Conclusion

Mean encoding is a powerful technique that captures the statistical relationship between categorical variables and the target, making it highly useful for predictive modeling. However, careful implementation with techniques like smoothing, cross-validation, and noise addition is essential to avoid overfitting and target leakage.

<a id='weight-of-evidence'></a>
# WEIGHT OF EVIDENCE

### Definition
Weight of Evidence (WoE) is a statistical technique used to encode categorical variables by quantifying their relationship with a binary target variable. It is widely used in risk modeling, such as credit scoring, as it provides a monotonic transformation that reflects the likelihood of a positive or negative outcome for each category.

#### Formula
For a given category \( c \):
\[
WoE(c) = \ln \left( \frac{\text{Proportion of Good Outcomes for } c}{\text{Proportion of Bad Outcomes for } c} \right)
\]
Where:
- **Good Outcomes**: Instances where the target is positive (e.g., default = 0 in credit scoring).
- **Bad Outcomes**: Instances where the target is negative (e.g., default = 1 in credit scoring).

#### Example
- Dataset:
  - `Category`: `A`, `B`, `C`
  - Target Proportions:
    - `A`: Good = 60%, Bad = 40%
    - `B`: Good = 30%, Bad = 70%
    - `C`: Good = 80%, Bad = 20%
- WoE Calculation:
  - \( WoE(A) = \ln \left( \frac{0.6}{0.4} \right) \approx 0.41 \)
  - \( WoE(B) = \ln \left( \frac{0.3}{0.7} \right) \approx -0.85 \)
  - \( WoE(C) = \ln \left( \frac{0.8}{0.2} \right) \approx 1.39 \)

---

### Advantages of Weight of Evidence (WoE)

1. **Monotonic Transformation**:
   - Ensures a monotonic relationship with the target variable, which is essential for logistic regression and credit risk models.

2. **Improves Interpretability**:
   - Encoded values represent the strength and direction of the relationship between the category and the target. Positive WoE indicates more "good" outcomes, and negative WoE indicates more "bad" outcomes.

3. **Scales with Model Requirements**:
   - Normalizes categorical variables, making them suitable for algorithms sensitive to value ranges (e.g., logistic regression).

4. **Handles Missing Values**:
   - Categories with missing data can be assigned their own WoE value, making it robust for incomplete datasets.

5. **Helps Reduce Dimensionality**:
   - Converts categorical variables into numerical values without requiring additional dummy variables, reducing the risk of multicollinearity.

---

### Limitations of Weight of Evidence (WoE)

1. **Requires a Binary Target**:
   - WoE is designed for binary classification problems. For multi-class targets, additional steps are required (e.g., creating one-vs-rest transformations).

2. **Not Suitable for Small Categories**:
   - Categories with few instances can result in extreme WoE values, leading to overfitting.

3. **Sensitive to Imbalanced Data**:
   - Imbalanced target distributions can distort WoE values, requiring adjustments like rebalancing or smoothing.

4. **Computational Overhead**:
   - Requires calculating proportions and logarithms for each category, which can be computationally expensive for large datasets.

5. **Loss of Interpretability in Multi-Level Aggregation**:
   - If WoE values are aggregated across multiple levels (e.g., hierarchies of categories), the interpretability can decrease.

6. **Risk of Target Leakage**:
   - Using the target variable to calculate WoE values without proper cross-validation can lead to overfitting and biased results.

---

### Techniques to Address Limitations

1. **Smoothing**:
   - Combine WoE values with global proportions to stabilize categories with few samples.

2. **Binning**:
   - Group categories with similar WoE values to avoid extreme values and improve stability.

3. **Cross-Validation**:
   - Use out-of-fold WoE calculations to prevent target leakage.

4. **Adjust for Imbalance**:
   - Resample the dataset or use weighted WoE to handle imbalanced targets.

---

### Example Scenario: Credit Risk Modeling

#### Dataset: Loan Default Prediction
- Variable: `Employment Status` with categories: `Employed`, `Unemployed`, `Retired`.
- Target: Default (1 = Yes, 0 = No).

#### WoE Encoding:
- **Category Statistics**:
  - `Employed`: Good = 85%, Bad = 15%.
  - `Unemployed`: Good = 40%, Bad = 60%.
  - `Retired`: Good = 70%, Bad = 30%.
- **WoE Values**:
  - `Employed`: \( WoE = \ln \left( \frac{0.85}{0.15} \right) \approx 1.73 \)
  - `Unemployed`: \( WoE = \ln \left( \frac{0.4}{0.6} \right) \approx -0.41 \)
  - `Retired`: \( WoE = \ln \left( \frac{0.7}{0.3} \right) \approx 0.85 \)

---

### Advantages in Use Case
- Ensures monotonicity, making it ideal for logistic regression.
- Helps identify strong predictors (e.g., `Employed` is highly associated with non-defaults).

### Limitations in Use Case
- **Small Categories**: If `Retired` had very few instances, its WoE might be unstable.
- **Imbalance**: If most loans were defaults, WoE values would require adjustment.

---

### Conclusion

Weight of Evidence encoding is a robust and interpretable technique for binary classification problems. It is particularly suited for risk modeling and logistic regression, where monotonicity and interpretability are critical. However, careful handling of small categories, target leakage, and imbalanced data is necessary to ensure effective and unbiased results.
