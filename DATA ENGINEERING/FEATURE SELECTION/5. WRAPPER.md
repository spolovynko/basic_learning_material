- [WRAPPER BASICS](#wrapper-basics)
- [FORWARD SELECTION](#forward-selection)
- [BACKWARD SELECTION](#backward-selection)
- [EXHAUSTIVE SEARCH](#exhaustive-search)

<a id= 'wrapper-basics'></a>
# Wrapper Feature Selection Methods

## Introduction
Wrapper methods are a type of feature selection technique that use a predictive model to evaluate the quality of different feature subsets. Unlike filter methods that rely on statistical metrics, wrapper methods directly assess the impact of feature combinations on a model’s performance. These methods are computationally expensive but often yield better results as they consider feature interactions.

## Types of Wrapper Feature Selection Methods
There are three main approaches within wrapper-based feature selection:

### 1. Step Forward Selection (SFS)
Step Forward Selection (SFS) is an iterative method that starts with an empty feature set and progressively adds features one at a time. At each step, the feature that improves model performance the most is added until no further improvement is observed or a predefined stopping criterion is met.

#### Algorithm:
1. Start with an empty feature subset.
2. Add the feature that improves the model’s performance the most.
3. Repeat step 2 until adding more features does not improve performance or a stopping condition is met (e.g., reaching a predefined number of features).

#### Pros:
- Efficient in finding a good subset.
- Suitable when dealing with a smaller number of features.

#### Cons:
- Can get stuck in a local optimum.
- Computationally expensive for large datasets.

### 2. Step Backward Selection (SBS)
Step Backward Selection (SBS) follows the opposite approach of SFS. It starts with all available features and removes the least significant one at each iteration until performance deteriorates significantly or a stopping criterion is reached.

#### Algorithm:
1. Start with all features in the dataset.
2. Remove the feature whose exclusion least affects model performance.
3. Repeat step 2 until performance significantly degrades or the desired number of features is reached.

#### Pros:
- Effective when many irrelevant features exist.
- Considers feature interactions from the beginning.

#### Cons:
- More computationally expensive than SFS.
- Can remove relevant features early, affecting final model performance.

### 3. Exhaustive Feature Selection
Exhaustive feature selection evaluates all possible feature combinations to determine the best subset based on model performance. It provides the most optimal feature set but is highly computationally intensive, making it impractical for large datasets.

#### Algorithm:
1. Generate all possible feature subsets.
2. Train a model for each subset and evaluate its performance.
3. Select the subset that yields the highest performance.

#### Pros:
- Guarantees the best-performing feature set.
- Evaluates all feature interactions comprehensively.

#### Cons:
- Extremely computationally expensive.
- Not feasible for large feature spaces.

## Conclusion
Wrapper methods provide high-quality feature selection results by directly optimizing for model performance. However, their computational cost must be considered:
- **Step Forward Selection** is useful for small feature sets and when adding features progressively makes sense.
- **Step Backward Selection** is effective when many irrelevant features are present but requires significant computation.
- **Exhaustive Selection** guarantees the best results but is impractical for large datasets.

Choosing the right wrapper method depends on the dataset size, computational constraints, and the need for feature interactions in model building.

<a id= 'forward-selection'></a>
# Step Forward Selection (SFS)

## Introduction
Step Forward Selection (SFS) is a sequential feature selection method that starts with an empty set of features and iteratively adds the most relevant feature at each step to improve model performance. It continues until a predefined stopping criterion is met. This method is widely used for feature selection in machine learning to reduce dimensionality while maintaining or improving model performance.

## How Step Forward Selection Works
### Algorithm:
1. Start with an empty feature set.
2. Train a model with each individual feature and select the one that provides the best performance.
3. Add the selected feature to the feature set.
4. Train models with the remaining features (one at a time along with the already selected features) and choose the one that improves performance the most.
5. Repeat step 4 until a stopping criterion is met.

## When to Stop the Search
Step Forward Selection does not run indefinitely. It must stop based on predefined conditions:

### 1. **Ideal Stopping Criteria**
   - **Performance Plateau:** If adding more features does not significantly improve model performance (e.g., accuracy, F1-score, or another metric), stop adding features.
   - **Overfitting Detection:** If additional features start decreasing validation performance due to overfitting, stop the process.
   - **Feature Limit:** Stop when a predefined maximum number of features is reached.
   - **Computational Constraints:** If adding more features exceeds acceptable computational time or resources, stop.

### 2. **MLXtend Implementation**
MLXtend provides an efficient implementation of Step Forward Selection through the `SequentialFeatureSelector` module. It allows users to define stopping criteria such as the number of selected features, cross-validation strategy, and performance metrics.

#### Example Implementation:
```python
from mlxtend.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Load dataset
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model
model = LogisticRegression()

# Perform Step Forward Selection
sfs = SequentialFeatureSelector(model,
                                k_features=3,  # Stop when selecting 3 features
                                forward=True,
                                floating=False,
                                scoring='accuracy',
                                cv=5)

sfs = sfs.fit(X_train, y_train)

# Selected feature indices
selected_features = sfs.k_feature_idx_
print("Selected features:", selected_features)
```

#### Key Parameters:
- `k_features`: Defines the number of features to select (stopping criterion).
- `forward=True`: Enables step forward selection.
- `floating=False`: Disables stepwise floating selection.
- `scoring='accuracy'`: Determines performance metric.
- `cv=5`: Uses 5-fold cross-validation for evaluation.

## Conclusion
Step Forward Selection is an effective method for selecting a compact set of relevant features. It stops when performance stabilizes, overfitting is detected, a feature limit is reached, or computational constraints arise. Using libraries like MLXtend simplifies its implementation, providing a structured approach to selecting optimal features for machine learning models.


<a id= 'backward-selection'></a>
# Backward Feature Selection (SBS)

## Introduction
Backward Feature Selection, also known as Step Backward Selection (SBS), is a feature selection method that starts with the full set of features and iteratively removes the least significant feature at each step. The goal is to identify a subset of features that optimally contribute to model performance.

## How Backward Feature Selection Works
### Algorithm:
1. Start with all available features.
2. Train the model using the full feature set and evaluate performance.
3. Remove the feature that has the least impact on performance (e.g., highest p-value, lowest feature importance, or minimal effect on accuracy).
4. Retrain the model with the reduced feature set and evaluate performance.
5. Repeat step 3 until a stopping criterion is met.

## When to Stop the Search
Step Backward Selection needs a well-defined stopping condition to avoid excessive feature removal.

### 1. **Ideal Stopping Criteria**
   - **Performance Drop:** Stop removing features when further removal significantly decreases model performance.
   - **Overfitting Avoidance:** If validation performance starts decreasing while training performance remains high, stop to prevent overfitting.
   - **Feature Limit:** Stop when a predefined minimum number of features is reached.
   - **Computational Efficiency:** Stop if removing more features does not provide a significant gain in computational efficiency.

### 2. **MLXtend Implementation**
MLXtend provides an easy way to implement Step Backward Selection using the `SequentialFeatureSelector` module, allowing for custom stopping conditions based on model performance.

#### Example Implementation:
```python
from mlxtend.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Load dataset
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model
model = LogisticRegression()

# Perform Step Backward Selection
sbs = SequentialFeatureSelector(model,
                                k_features=2,  # Stop when 2 features remain
                                forward=False,
                                floating=False,
                                scoring='accuracy',
                                cv=5)

sbs = sbs.fit(X_train, y_train)

# Selected feature indices
selected_features = sbs.k_feature_idx_
print("Selected features:", selected_features)
```

#### Key Parameters:
- `k_features`: Defines the number of features to retain (stopping criterion).
- `forward=False`: Enables step backward selection.
- `floating=False`: Disables stepwise floating selection.
- `scoring='accuracy'`: Determines performance metric.
- `cv=5`: Uses 5-fold cross-validation for evaluation.

## Conclusion
Step Backward Selection is a useful method for refining feature sets by iteratively removing the least impactful features. The selection process stops based on performance deterioration, overfitting concerns, feature constraints, or computational considerations. Using MLXtend simplifies implementation, making it an efficient tool for optimizing feature subsets in machine learning models.


<a id= 'exhaustive-search'></a>
# Exhaustive Feature Selection

## Introduction
Exhaustive Feature Selection is a brute-force feature selection method that evaluates all possible feature combinations to identify the best-performing subset. Unlike stepwise methods (forward or backward selection), this approach guarantees the best subset by testing every possible feature combination, making it computationally expensive but highly effective.

## How Exhaustive Feature Selection Works
### Algorithm:
1. Generate all possible feature subsets from the dataset.
2. Train a model using each subset and evaluate its performance using a chosen metric (e.g., accuracy, F1-score, RMSE).
3. Identify the subset that yields the highest performance.
4. Select that subset as the optimal feature set for the model.

## Advantages and Disadvantages
### **Pros:**
- **Guaranteed Optimal Subset:** Since all combinations are tested, the best feature subset is selected.
- **Considers Feature Interactions:** Unlike stepwise methods, it does not suffer from locally optimal decisions.
- **Model-Agnostic:** Works with any predictive model.

### **Cons:**
- **Extremely Computationally Expensive:** For a dataset with `n` features, it evaluates `2^n - 1` subsets.
- **Not Feasible for Large Feature Sets:** The complexity increases exponentially, making it impractical for high-dimensional datasets.
- **Time-Consuming:** Training and evaluating models for every subset takes significant computational resources.

## When to Use Exhaustive Search
Exhaustive Feature Selection is useful in the following scenarios:
- When the number of features is small (e.g., fewer than 15-20 features).
- When computational resources are not a constraint.
- When achieving the absolute best feature subset is a priority.
- When feature interactions are critical to model performance.

## MLXtend Implementation
MLXtend provides an efficient implementation of Exhaustive Feature Selection through `ExhaustiveFeatureSelector`, allowing users to apply it within scikit-learn pipelines.

### **Example Implementation:**
```python
from mlxtend.feature_selection import ExhaustiveFeatureSelector
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Load dataset
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model
model = LogisticRegression()

# Perform Exhaustive Feature Selection
efs = ExhaustiveFeatureSelector(model,
                                min_features=1,
                                max_features=3,  # Limits max subset size to control computation
                                scoring='accuracy',
                                cv=5)

efs = efs.fit(X_train, y_train)

# Selected feature indices
selected_features = efs.best_idx_
print("Selected features:", selected_features)
```

### **Key Parameters:**
- `min_features`: Minimum number of features in the search.
- `max_features`: Maximum number of features to consider (helps limit computation).
- `scoring='accuracy'`: Metric to evaluate feature subsets.
- `cv=5`: Uses 5-fold cross-validation for evaluation.

## Conclusion
Exhaustive Feature Selection is a powerful method that guarantees optimal feature subset selection by testing all possible combinations. However, its computational cost makes it impractical for large datasets. When feasible, it provides the best possible feature selection results, ensuring maximum model performance.

