## Common Classification Metrics

Below is a table summarizing frequently used metrics for **classification** tasks (binary, multiclass, or multilabel), along with their definitions and typical usage scenarios.

| **Metric**        | **What it Calculates**                                                                                        | **When to Use**                                                                                                 |
|-------------------|---------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| **Accuracy**      | Fraction of correct predictions: \(\frac{\text{Number of correct predictions}}{\text{Total predictions}}\).  | When classes are **balanced** and you want a straightforward measure of overall correctness.                    |
| **Precision**     | Among predicted positives, how many are truly positive? \(\frac{\text{TP}}{\text{TP} + \text{FP}}\).          | When **false positives** are costly (e.g., spam detection, medical tests where you want fewer false alarms).     |
| **Recall**        | Among actual positives, how many did we correctly identify? \(\frac{\text{TP}}{\text{TP} + \text{FN}}\).      | When **false negatives** are costly (e.g., detecting diseases, fraud, or other “must detect” conditions).        |
| **F1 Score**      | Harmonic mean of Precision and Recall: \(2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}\). | When both **precision and recall** are important and you want a single-number summary.                           |
| **ROC AUC**       | Area Under the Receiver Operating Characteristic curve. Measures how well the model ranks positives over negatives. | When classes are somewhat **balanced**, and you need to evaluate overall ranking performance.                     |
| **Precision-Recall AUC** | Area Under the Precision-Recall curve. Indicates trade-off between Precision and Recall at different thresholds. | When **classes are imbalanced**, and you want to highlight performance on the positive/minority class.           |
| **Specificity**   | True Negative Rate: \(\frac{\text{TN}}{\text{TN} + \text{FP}}\).                                              | When **true negatives** are critical to measure, or as a complement to Recall (also known as Sensitivity).       |
| **Log Loss** (Cross-Entropy) | Negative log-likelihood of predicted probabilities. Heavily penalizes confident misclassifications.   | When you want to assess the **quality of probabilistic outputs** (not just discrete labels).                      |

### Key Takeaways
1. **Accuracy** is simple but can be **misleading** on highly imbalanced datasets.  
2. **Precision** and **Recall** focus on specific types of errors, important in tasks with severe false positives or false negatives.  
3. **F1 Score** balances **Precision** and **Recall**, useful if both metrics are crucial.  
4. **ROC AUC** measures the ranking performance across different thresholds, but it may not be optimal for **heavily imbalanced** data.  
5. **Precision-Recall AUC** is better for **imbalanced** scenarios, emphasizing performance on the positive class.  
6. **Specificity** measures how well you identify negatives, complementary to **Recall** (or Sensitivity) for a comprehensive view.  
7. **Log Loss** evaluates the **quality of predicted probabilities**, penalizing overconfident and incorrect predictions more heavily.

Choose the metric(s) that align with your **application goals** and **class distribution**. In practice, it’s often helpful to track multiple metrics to get a more complete picture of model performance.
