# CONTENT

- [ARCHITECTURE](#architecture)
- [INPUT/OUTPUT](#input-output)
- [MODEL IMPROVEMENT](#model-improvement)
- [ACTIVATION FUNCTION](#activation)
- [OPTIMISATION](#optimisation)
- [EVALUATION](#evaluation)
 <a id='architecture'></a>

# Typical Architecture of a Regression Neural Network

The word **typical** is intentional because there are nearly infinite ways to structure a neural network. However, the following provides a good starting point for problems where you want to predict a continuous value (e.g., a house price) based on numerical features (e.g., number of bedrooms, bathrooms, car spaces).

| **Hyperparameter**            | **Typical value**                                                                                                                                                                                                  |
|-------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Input layer shape**         | Same as the number of features (e.g., 3 for # bedrooms, # bathrooms, # car spaces).                                                                                                                                |
| **Hidden layer(s)**           | Problem-specific. Minimum = 1, maximum = unlimited. A common starting point is 1–3 hidden layers.                                                                                                                   |
| **Neurons per hidden layer**  | Problem-specific. Often 10–100 neurons per layer is a good initial range.                                                                                                                                           |
| **Output layer shape**        | Same as the desired prediction shape (e.g., 1 neuron for a single house price).                                                                                                                                      |
| **Hidden activation**         | Commonly [ReLU](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning) for faster convergence and good performance.                                                                        |
| **Output activation**         | Typically **None** (linear) for pure regression. Possible to use ReLU/logistic/tanh if there's a reason to constrain outputs.                                                                                      |
| **Loss function**             | [MSE](https://en.wikipedia.org/wiki/Mean_squared_error) (Mean Squared Error), [MAE](https://en.wikipedia.org/wiki/Mean_absolute_error) (Mean Absolute Error), or **Huber** (robust to outliers).                        |
| **Optimizer**                 | [SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD) (Stochastic Gradient Descent) or [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) are common defaults.                                              |

## Why These Typical Choices?

1. **Input Layer Shape**  
   - Matches exactly the number of features in your dataset. For example, if your dataset has 3 numerical features (bedrooms, bathrooms, car spaces), the input layer shape is 3.

2. **Hidden Layers and Neurons**  
   - **Number of Hidden Layers**: At least one hidden layer lets the model learn non-linear relationships. You can stack more layers for more complex tasks.  
   - **Neurons per Layer**: Often, 10–100 neurons is a good start. This can be increased or decreased based on performance and computational resources.

3. **Output Layer Shape**  
   - If you're predicting a single numeric value (e.g., house price), you'll likely use just 1 neuron.  
   - If you have multiple numeric outputs (e.g., predicting multiple targets simultaneously), match that number accordingly.

4. **Hidden Activation**  
   - [ReLU](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning) is popular because it's computationally efficient and helps mitigate the vanishing gradient problem.

5. **Output Activation**  
   - Often **none** (linear) for most regression tasks.  
   - In cases where you need to constrain outputs (e.g., only positive), you might use ReLU or another suitable activation.

6. **Loss Function**  
   - **MSE** (Mean Squared Error) is a standard choice that penalizes larger errors more heavily.  
   - **MAE** (Mean Absolute Error) is less sensitive to large outliers.  
   - **Huber** loss is a combination that is robust to outliers yet differentiable near zero error.

7. **Optimizer**  
   - **SGD** (with or without momentum) is simple and can yield great results if properly tuned.  
   - **Adam** adapts the learning rate for each parameter and often converges faster without as much hyperparameter tuning.

## Example in Code

Below is a minimal example using a Keras-like API (TensorFlow). Note that this is just a generic template—your actual code may differ.

```python
import tensorflow as tf
from tensorflow.keras import layers, models

# Create a sequential model
model = models.Sequential()

# Input layer + first hidden layer
model.add(layers.Dense(32, activation='relu', input_shape=(3,)))  # e.g., 3 features

# Second hidden layer
model.add(layers.Dense(32, activation='relu'))

# Output layer (1 neuron for a single regression value)
model.add(layers.Dense(1))

# Compile the model with chosen loss and optimizer
model.compile(
    loss='mse',        # could also try 'mae' or Huber loss
    optimizer='adam',  # could also try 'sgd'
    metrics=['mae', 'mse']
)

model.summary()  # Check the layers and parameters
```
<a id='input-output'></a>
# Input and Output Shapes for a Regression Problem

In a regression problem, you’re predicting **continuous** value(s). The **input shape** indicates how many and what type of features go into the model, while the **output shape** specifies how many continuous values the model will output.

---

## 1. Input Shape

### 1.1 Number of Features (Dimensionality)

- **Single Feature (Univariate Input):**  
  If you have only one numeric feature, each sample is represented by a single value.  
  - Example scenario: Predicting sales based solely on temperature.

- **Multiple Features (Multivariate Input):**  
  More commonly, you have several numeric features (e.g., bedrooms, bathrooms, square footage).  
  - Example scenario: Predicting house price using 3 features: \[bedrooms, bathrooms, car_spaces\].  
  - If you have 10 features, then each sample has 10 numbers associated with it.

### 1.2 Data Format

When data is processed in batches:
- You have \(N\) samples, each with \(d\) features.
- Conceptually, the data shape is \((N, d)\).
- Many frameworks require only the shape of **one** sample (i.e., `(d,)`), and handle the batch dimension automatically.

### 1.3 Special Cases (Optional)

- **Image Data:**  
  The input might have height, width, and channels (for example, `(height, width, channels)`).
- **Sequence Data:**  
  For time series, the input might be `(timesteps, features_per_timestep)`.

However, for **typical numeric/tabular regression**, your input shape is generally just `(number_of_features,)` for each sample.

---

## 2. Output Shape

### 2.1 Single Continuous Value

For most regression tasks, you predict exactly **one number**:
- Example: House price as a single dollar amount.
- Each sample’s output is a single scalar (one number).

### 2.2 Multiple Continuous Values

In some cases, you want to predict **more than one** continuous value per sample:
- Example 1: Predicting **height** and **weight** simultaneously from certain features.
- Example 2: Predicting **price** and **expected rental income** for a house.
- Each sample’s output is a vector (e.g., two or more numbers).

### 2.3 Shape and Squeezing

- If you predict a single output, some frameworks may automatically squeeze shapes from `(N, 1)` to `(N,)`.
- Multiple outputs remain `(N, number_of_outputs)` (e.g., two outputs become `(N, 2)`).

---

## 3. Putting It All Together

### Example of a Simple Case

- **Input:** 3 features (e.g., bedrooms, bathrooms, car_spaces)  
  - Input shape: `(3,)` for each individual sample.
- **Output:** 1 predicted house price  
  - Output shape: `(1,)` per sample.

### Example with Multiple Outputs

- **Input:** Same 3 features  
  - Input shape: `(3,)`.
- **Output:** 2 values (house price and expected rental income)  
  - Output shape: `(2,)` per sample.

---

## 4. Key Takeaways

1. **Match the Input Shape to the Number of Features:**  
   - If you have \(d\) features, your input shape is `(d,)`.

2. **Match the Output Shape to the Number of Continuous Values You Need:**  
   - Single regression target ⇒ `(1,)`.  
   - Multiple regression targets ⇒ `(M,)`, where \(M\) is the number of targets.

3. **Batch vs. Single Sample:**  
   - Internally, data typically has a first dimension (batch size), so the full data shape is `(batch_size, d)`.  
   - Most libraries only ask you to define the shape of **one** sample, not the entire batch.

4. **Simplify, Then Scale Up:**  
   - Start with a small number of features and/or outputs, and expand if your problem requires it.

By carefully defining your **input shape** and **output shape**, you ensure your neural network is architected correctly to handle the data dimensions needed for regression tasks.

<a id='model-improvement'></a>
# How to Improve a Regression Neural Network

Below are strategies to enhance the performance and generalization of a neural network designed for **regression** tasks. While these methods apply broadly to many neural network architectures, the examples and recommendations focus on **regression** scenarios, where the goal is to predict one or more continuous values.

---

## 1. Adding More Layers (Increasing Depth)

### 1.1 Why Add Layers?
- Deeper networks can capture more complex functions and learn higher-level abstractions from your data.
- Each added layer can uncover patterns that shallower networks might miss.

### 1.2 Potential Downsides
- More layers can lead to **overfitting**—the network may memorize the training data instead of learning patterns that generalize.
- Training becomes more computationally expensive and prone to vanishing or exploding gradients, although techniques like batch normalization and skip connections can help.

### 1.3 Practical Tips
- Start with a modest depth (1–3 hidden layers) for a basic regression problem.
- Add layers incrementally and monitor validation performance to see if the extra depth is actually helping.
- Consider using **batch normalization** after each layer if training becomes unstable.

---

## 2. Increasing the Number of Hidden Units (Increasing Width)

### 2.1 Why Increase Hidden Units?
- More neurons in each layer can capture finer-grained patterns in the data.
- Often easier to tune than adding many layers.

### 2.2 Potential Downsides
- Increasing the width significantly increases the number of parameters, which can also lead to **overfitting** and higher computational cost.
- The model may require careful regularization (e.g., L2 weight decay, dropout) if it becomes too large.

### 2.3 Practical Tips
- Double the number of neurons in the hidden layers if you see signs of underfitting (the model is too simple).
- Use dropout or weight decay if you notice overfitting as you add neurons.

---

## 3. Changing the Activation Function

### 3.1 Common Activations for Regression

1. **ReLU (Rectified Linear Unit)**
   - `ReLU(x) = max(0, x)`
   - **Pros:** Fast to compute, helps mitigate vanishing gradients, typically a strong default for hidden layers.
   - **Cons:** Can lead to “dead neurons” if many outputs become zero (especially if learning rate is high).

2. **Leaky ReLU / ELU**
   - Variants that allow small negative outputs to prevent “dying ReLUs.”
   - **Pros:** Often more robust than standard ReLU.
   - **Cons:** Slightly more complex to implement and compute.

3. **Linear (No Activation)**
   - For **output** layer in regression, you often use a linear activation so you can predict any real value.
   - If you know your target is always positive, you might consider a ReLU output, but typically regression outputs are linear.

### 3.2 When to Switch Activations?
- If your model saturates or you observe very slow learning, try switching from **ReLU** to **Leaky ReLU** or **ELU**.
- For **output**, if you need bounded outputs (e.g., 0 to 1), you might use a sigmoid activation, but that’s less common in general regression.

---

## 4. Changing the Optimization Function

### 4.1 Common Optimizers for Regression

1. **SGD (Stochastic Gradient Descent)**  
   - **Pros:** Well-understood; can lead to good generalization with careful tuning (momentum, learning rate schedules).  
   - **Cons:** Tuning learning rate and momentum can be tedious; can converge slowly if features aren’t well-scaled.

2. **Adam**  
   - **Pros:** Adaptive learning rate per parameter; often converges quickly; good out-of-the-box performance.  
   - **Cons:** May require tuning the default hyperparameters (learning rate, beta1, beta2) for best results; can overfit if not watched carefully.

3. **AdamW**  
   - **Pros:** Similar to Adam but decouples weight decay; better for regularization than vanilla Adam.  
   - **Cons:** Slightly more complex setup, but often a strict improvement over Adam if using weight decay.

### 4.2 When to Switch Optimizers?
- If **SGD** is too slow to converge or you struggle with manual learning-rate schedules, try **Adam** or **AdamW**.
- If **Adam** quickly converges but generalization suffers, experiment with **SGD + momentum** or use **AdamW** with weight decay.

---

## 5. Changing the Learning Rate

### 5.1 Why It Matters
- **Learning rate** is one of the most critical hyperparameters in training neural networks.
- A **high** learning rate can cause the model to diverge or overshoot minima.
- A **low** learning rate can lead to very slow convergence or getting stuck in local minima.

### 5.2 Strategies
- **Learning Rate Schedules**: Start high and reduce over time (e.g., step decay, exponential decay, or cosine decay).
- **Learning Rate Finder**: A tool or method to scan a range of learning rates and see which leads to fast improvement versus divergence.
- **Warm Restarts**: Periodically reset or “warm” the learning rate to encourage the model to escape suboptimal minima.

### 5.3 Practical Tips
- Begin with a moderate learning rate (e.g., 1e-3 for Adam) and adjust up or down by factors of 2 or 10 to find a sweet spot.
- Track both training loss and validation loss. If training loss rapidly decreases but validation loss diverges, your learning rate might be too high (or regularization too low).

---

## 6. Fitting More Data

### 6.1 Why More Data Helps
- More data typically **reduces overfitting** and improves the model’s ability to generalize.
- Collecting or synthesizing more data often yields bigger gains in performance than fine-tuning hyperparameters.

### 6.2 Downsides
- Large datasets can be expensive to obtain, label, or store.
- Training time may increase significantly.

### 6.3 Practical Tips
- Consider data augmentation (if applicable, e.g., for images).
- Look for open data sources or synthetic data generation if you can’t collect real data easily.
- Use cross-validation to make the most of existing data.

---

## 7. Fitting for Longer (Increasing Epochs)

### 7.1 Why Train Longer?
- The model might not have converged fully if underfitting remains high.
- Additional epochs allow the network more opportunities to tune weights toward reducing loss.

### 7.2 Caution
- Training too long can lead to **overfitting** (training loss keeps going down while validation loss starts increasing).
- Implement **early stopping** to halt training once the validation loss stops improving.

### 7.3 Practical Tips
- Track validation metrics (e.g., validation MSE) during training.
- Use **early stopping** with a patience parameter (e.g., wait for 5–10 epochs of no improvement before stopping).

---

## 8. Summary of Improvement Strategies

1. **Architecture Adjustments:**
   - **Increase depth** (more layers) or **width** (more neurons) to capture more complex patterns.  
   - Watch for overfitting.

2. **Activation Function Choices:**
   - Stick with **ReLU** for hidden layers by default; consider **Leaky ReLU** or **ELU** if ReLU “dead neurons” occur.  
   - Keep the **output** layer typically **linear** for general regression.

3. **Optimizer Selection:**
   - **SGD** (with momentum) for strong generalization, **Adam/AdamW** for faster convergence.  
   - Regularly tune or monitor learning rate.

4. **Learning Rate Tuning:**
   - One of the most crucial hyperparameters for convergence.  
   - Use schedules or methods to find the optimal rate.

5. **Data Improvements:**
   - More data typically helps generalization.  
   - Use augmentation or additional data sources if possible.

6. **Training Duration:**
   - Train for more epochs if you see underfitting; monitor validation metrics to avoid overfitting.  
   - Consider **early stopping** to halt at the right time.

By systematically applying these strategies and carefully monitoring training and validation performance, you can iteratively refine your regression network to achieve better predictive accuracy and generalization.

 <a id='activation'></a>
 # A Deeper Look at Activation Functions (Especially for Regression)

An **activation function** introduces nonlinearity to a neural network, allowing it to learn complex patterns. Without activation functions, a neural network would essentially be a linear model, regardless of how many layers it has. Below is an overview of commonly used activation functions, their properties, and specific considerations for **regression tasks**.

---

## 1. ReLU (Rectified Linear Unit)

**Definition:**
\[
\text{ReLU}(x) = \max(0, x)
\]

- **Pros:**
  - Computationally efficient (simple max operation).
  - Less prone to the vanishing gradient problem than sigmoid or tanh.
  - Works well as the default activation in hidden layers.

- **Cons:**
  - Can cause “dying ReLUs”—once a neuron outputs zero, it may stay at zero if gradients don’t move it away again (particularly if the learning rate is too high).
  - Outputs are unbounded on the positive side, which usually is fine for hidden layers but can sometimes cause large activations.

- **Use in Regression:**
  - Typically used in **hidden layers**.
  - **Output layer** is usually *not* ReLU for general regression unless you need to ensure outputs are **non-negative** (e.g., a price that can’t be negative).  

---

## 2. Leaky ReLU (and Other Variants like ELU, SELU)

### 2.1 Leaky ReLU

**Definition:**
\[
\text{LeakyReLU}(x) =
\begin{cases}
x, & x \ge 0 \\
\alpha x, & x < 0
\end{cases}
\]
where \(\alpha\) is a small slope for negative \(x\) (e.g., 0.01).

- **Pros:**
  - Prevents neurons from completely “dying,” since the negative side has a slight slope.
  - Often helps maintain a small gradient for negative inputs.

- **Cons:**
  - Adds a small hyperparameter \(\alpha\) to tune.
  - Still unbounded in the positive direction.

- **Use in Regression:**
  - A good alternative to ReLU if you notice many neurons saturating at zero.
  - Still typically used in hidden layers, not in the output layer (unless you specifically want a non-negative output).

### 2.2 ELU (Exponential Linear Unit)

**Definition:**
\[
\text{ELU}(x) =
\begin{cases}
x, & x \ge 0 \\
\alpha(e^x - 1), & x < 0
\end{cases}
\]
where \(\alpha\) is a positive constant (e.g., 1.0).

- **Pros:**
  - Negative inputs push outputs towards \(-\alpha\), helping the neuron’s output center around zero.
  - Potentially better performance in practice than ReLU or Leaky ReLU for some tasks.

- **Cons:**
  - More computationally expensive than ReLU (due to exponent).
  - Adds a hyperparameter \(\alpha\).

- **Use in Regression:**
  - Often found in hidden layers for tasks where zero-centered outputs help convergence.
  - Rarely used in output layers for general regression due to the negative bounding and exponential behavior.

---

## 3. Sigmoid (Logistic Function)

**Definition:**
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]

- **Pros:**
  - Outputs range from 0 to 1 (bounded). Useful when modeling probabilities.

- **Cons:**
  - Can cause the **vanishing gradient** problem in deep networks because gradients become very small as \(x\) moves away from 0.
  - Not typically suitable for large-range regression outputs because it saturates.

- **Use in Regression:**
  - Rarely used in hidden layers because of slow convergence and vanishing gradients.
  - Potentially useful in the **output layer** if you need to constrain the output to \([0,1]\) (e.g., a fractional or percentage-like prediction).

---

## 4. Tanh

**Definition:**
\[
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]

- **Pros:**
  - Outputs range from -1 to 1.
  - Zero-centered output can be beneficial for some tasks (less shifting required in gradients).

- **Cons:**
  - Similar vanishing gradient issues as sigmoid when \(x\) is large (positive or negative).
  - Less popular than ReLU/Leaky ReLU for hidden layers in deep networks.

- **Use in Regression:**
  - Typically not in hidden layers for large networks (ReLU or variants are preferred).
  - Could be used for **output** if your target is constrained to \([-1, 1]\) or if you normalize your target to that range.

---

## 5. Linear (No Activation)

**Definition:**
\[
f(x) = x
\]

- **Pros:**
  - This is the **standard choice** for the **output layer** in regression, allowing any real value as output.
  - No gradient saturation issues.

- **Cons:**
  - Provides no nonlinearity (only use it in the output layer if you need unbounded outputs).

- **Use in Regression:**
  - **The most common activation in the output layer** for a typical regression problem (e.g., predicting price, temperature, or any real-number quantity).

---

## 6. Choosing the Right Activation for Regression

1. **Hidden Layers**  
   - **Default:** **ReLU** is the usual first pick due to ease of use and strong performance.  
   - **Alternatives:** **Leaky ReLU** or **ELU** may help if ReLU neurons frequently saturate at zero.  
   - **Avoid** Sigmoid/tanh in deeper nets if performance or training speed is an issue, unless there’s a specific reason (e.g., if your input data or hidden representation is normalized around 0).

2. **Output Layer**  
   - **Linear** (no activation) is standard for most regression tasks (e.g., predicting house prices, stock values, etc.).  
   - If you need to strictly output non-negative values, ReLU could be used in the output layer.  
   - If your output must be within \([0,1]\), **sigmoid** is an option, and if it must be in \([-1,1]\), **tanh** might be suitable.

3. **Performance and Practical Considerations**  
   - Monitor **loss** and **validation error**. If training is slow or seems stuck, try switching from ReLU to **Leaky ReLU** or **ELU**.  
   - Activation changes alone can’t fix all issues; pairing the right activation with appropriate **initialization**, **optimizer**, and **learning rate** is crucial.  

---

## 7. Summary

- **ReLU** is often best for **hidden layers** in regression problems due to its simplicity and effectiveness.
- **Leaky ReLU/ELU** can help if ReLU neurons are dying or convergence is slow.
- **Sigmoid** and **tanh** are less common in hidden layers for modern regression tasks but can be used if you have bounded or normalized data.
- **Linear (no activation)** is usually the go-to choice for **regression outputs**, unless there’s a reason to constrain your output range.

By selecting an activation function that aligns with your data properties and task requirements, you help ensure that your regression model can learn effectively and produce meaningful predictions. 

 <a id='optimisation'></a>
 # A Deeper Look at Optimization in Neural Networks (Especially for Regression)

**Optimization** is the process of adjusting your network’s parameters (weights and biases) to minimize a **loss function**. In regression tasks, common loss functions include **Mean Squared Error (MSE)**, **Mean Absolute Error (MAE)**, or the **Huber** loss. Below is a detailed overview of how optimization works and the most commonly used algorithms.

---

## 1. Overview of the Optimization Process

1. **Forward Pass:**  
   - The input data is fed through the network layer by layer.  
   - Each neuron performs a weighted sum of its inputs plus a bias, applies an activation function, and passes the result on to the next layer.  
   - At the final (output) layer, the network produces a prediction (e.g., a house price).

2. **Loss Computation:**  
   - The prediction is compared to the true value via a loss function (e.g., MSE).  
   - The loss is a single scalar that indicates how far off the prediction is from the ground truth.

3. **Backward Pass (Backpropagation):**  
   - Gradients of the loss with respect to each parameter are computed using the **chain rule**.  
   - This tells us how each weight and bias influenced the final loss.

4. **Parameter Update (Weight Adjustment):**  
   - An **optimizer** uses these gradients to update the parameters in a way that (ideally) decreases the loss.  
   - This step often looks like:  
     \[
     w \leftarrow w - \eta \frac{\partial \text{Loss}}{\partial w}
     \]
     where \( w \) is a weight parameter and \( \eta \) is the **learning rate**.

5. **Repeat:**  
   - The process repeats for multiple **epochs** over the training dataset.  
   - With each iteration, the network should gradually fit the data better (training loss decreases).

---

## 2. Common Optimizers

### 2.1 SGD (Stochastic Gradient Descent)

- **How It Works:**
  - Updates parameters using the gradient computed from a **mini-batch** of the training data.
  - Formula (simplified):
    \[
    w \leftarrow w - \eta \, g
    \]
    where \( g \) is the gradient for a mini-batch.

- **Pros:**
  - Simple and widely understood.
  - Often yields good **generalization** performance with the right learning rate schedule.

- **Cons:**
  - May converge slowly, especially if the features aren’t scaled properly.
  - Sensitive to the choice of learning rate (too large can diverge, too small can stall).

- **Enhancements (Momentum, Nesterov, etc.):**
  - **Momentum** helps accelerate SGD in the correct direction and dampen oscillations.
  - **Nesterov** momentum looks ahead before computing the gradient, providing an additional correction term.

- **When to Use (in Regression):**
  - Works well if you have a well-tuned learning rate and momentum.
  - Particularly common in large-scale tasks like image recognition, but can be just as effective for tabular regression with proper tuning.

---

### 2.2 Adam (Adaptive Moment Estimation)

- **How It Works:**
  - Calculates individual adaptive learning rates for each parameter by keeping track of:
    1. An exponentially decaying average of past gradients (first moment).
    2. An exponentially decaying average of past squared gradients (second moment).
  - Adjusts each weight’s step size based on these averages.

- **Pros:**
  - Often converges **faster** than plain SGD for many problems.
  - Less sensitive to initial learning rate choice (though it still matters).

- **Cons:**
  - Can sometimes lead to worse **generalization** than SGD if not carefully tuned.
  - Default hyperparameters (\(\beta_1 = 0.9, \beta_2 = 0.999\), etc.) can require tweaking for best results.

- **Variants (AdamW, AMSGrad, etc.):**
  - **AdamW** decouples weight decay (L2 regularization) from the gradient update, improving generalization.
  - **AMSGrad** ensures the learning rate never increases by taking the maximum of past squared gradients.

- **When to Use (in Regression):**
  - A strong **default** choice if you want quick results.
  - Often good if you have messy or **sparse** data, as the adaptive nature can handle varied feature scales.

---

### 2.3 RMSProp

- **How It Works:**
  - Similar to Adam’s second moment approach.  
  - Maintains an exponentially decaying average of squared gradients to adapt the learning rate for each parameter.
- **Pros:**
  - Works well on non-stationary problems (where input data statistics change over time).
  - Commonly used in recurrent neural networks and reinforcement learning.
- **Cons:**
  - Doesn’t incorporate the first moment (average of raw gradients) by default.
- **When to Use (in Regression):**
  - Another adaptive option if Adam isn’t performing well.
  - Particularly useful in sequences or time-series tasks with non-stationary data.

---

### 2.4 Choosing an Optimizer

1. **SGD** (with momentum):
   - If you prefer simpler updates, want potential better generalization, and can handle manual scheduling of the learning rate.

2. **Adam / AdamW**:
   - If you want faster convergence and adaptiveness.
   - AdamW is preferable if you also want well-defined weight decay regularization.

3. **RMSProp**:
   - If you’re dealing with non-stationary data or certain specialized tasks.

In **regression**, there is no universal “best” optimizer—experiment with a few, monitor validation loss, and pick the one yielding the best trade-off between speed of convergence and final performance.

---

## 3. The Role of the Learning Rate

### 3.1 Why It’s Crucial

- The **learning rate (\(\eta\))** controls **how big** the parameter update steps are.
- If it’s **too high**, the model may overshoot minima and fail to converge (leading to unstable or diverging loss).
- If it’s **too low**, training can be very slow or get stuck in suboptimal regions.

### 3.2 Learning Rate Schedules

1. **Step Decay**:
   - Reduce the learning rate by a factor (e.g., 10) after a certain number of epochs.

2. **Exponential Decay**:
   - Continuously decrease the learning rate by a constant factor each epoch or iteration.

3. **Cosine Decay / Warm Restarts**:
   - Cosine-based schedule that gradually reduces and then restarts the learning rate to escape local minima.

4. **Adaptive (Built-In) Methods**:
   - Optimizers like Adam, RMSProp, or Adadelta inherently adapt the learning rate at each parameter dimension.

### 3.3 Best Practices for Regression

- Start with a moderate learning rate (e.g., **0.001** for Adam, **0.01** for SGD + momentum).
- Monitor the **loss** curve:
  - If the loss oscillates wildly or diverges, **lower** the learning rate.
  - If the loss decreases very slowly, consider **increasing** it or adding a schedule.
- Use **learning rate scheduling** if you see training plateau at some point—this can help find a finer local minimum.

---

## 4. Practical Tips for Regression Optimization

1. **Normalize or Scale Your Inputs**:  
   - Scaling features (e.g., using **StandardScaler** or **MinMaxScaler**) often helps gradient-based optimizers converge faster.

2. **Try Multiple Optimizers**:  
   - **SGD** vs. **Adam** vs. **AdamW**—some tasks see dramatic differences, so it’s worth experimenting.

3. **Use Validation Curves**:  
   - Track **validation loss** (e.g., MSE or MAE) to see if your model is overfitting or underfitting.  
   - Compare different learning rates or optimizers by how fast and how low the validation loss goes.

4. **Combine with Regularization**:  
   - **Weight decay**, **dropout**, or **batch normalization** can improve generalization and stabilize optimization.  
   - Overly large networks optimized with a powerful optimizer (like Adam) can easily overfit if not regularized.

5. **Watch Out for Overfitting**:  
   - If training loss keeps decreasing but validation loss stops decreasing or starts to increase, you might be overfitting.  
   - Consider using **early stopping** or collecting more data.

---

## 5. Summary

- **Optimization** is a crucial aspect of training neural networks for **regression**.  
- **SGD** (with momentum) is a classic approach known for good final performance but may require meticulous tuning of learning rates.  
- **Adam/AdamW** is a more adaptive method that can converge quickly, though it sometimes needs careful hyperparameter adjustments.  
- The **learning rate** is one of the most important hyperparameters; using schedules or adaptive techniques can help.  
- Always check your **validation** performance to ensure you aren’t just memorizing the training data.

By systematically experimenting with different optimizers, tuning learning rates, and employing regularization when needed, you can guide your regression network to better convergence and generalization.

<a id='evaluation'></a>
# Methods to Evaluate a Regression Model

Evaluating a regression model involves measuring how accurately its predictions match real-world (or test) data. Below are common **quantitative metrics** and **qualitative methods** to assess a model’s performance and overall suitability for a regression task.

---

## 1. Quantitative Metrics

### 1.1 Mean Squared Error (MSE)
\[
\text{MSE} = \frac{1}{N}\sum_{i=1}^{N} (y_i - \hat{y}_i)^2
\]
- **Interpretation:** Measures the average of the squared differences between predicted (\(\hat{y}_i\)) and actual (\(y_i\)) values.  
- **Pros:** Heavily penalizes larger errors (due to the squaring).  
- **Cons:** Can be more sensitive to outliers; the units are squared relative to the target variable.

### 1.2 Root Mean Squared Error (RMSE)
\[
\text{RMSE} = \sqrt{\text{MSE}}
\]
- **Interpretation:** Square root of the MSE, bringing the metric back to the same scale as the target variable.  
- **Pros:** More intuitive than MSE for many practitioners because it’s in the same units as the original data.  
- **Cons:** Like MSE, it still penalizes larger errors more heavily.

### 1.3 Mean Absolute Error (MAE)
\[
\text{MAE} = \frac{1}{N}\sum_{i=1}^{N} \left| y_i - \hat{y}_i \right|
\]
- **Interpretation:** Measures the average magnitude of errors without considering their direction (positive or negative).  
- **Pros:** Less sensitive to outliers than MSE/RMSE.  
- **Cons:** Doesn’t square the errors, so large errors are treated proportionally rather than disproportionately.

### 1.4 Mean Absolute Percentage Error (MAPE)
\[
\text{MAPE} = \frac{100\%}{N} \sum_{i=1}^{N} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
\]
- **Interpretation:** Expresses the error as a percentage of the actual values.  
- **Pros:** Easily interpretable in terms of percentage error.  
- **Cons:** Can be undefined if \(y_i = 0\), and overemphasizes errors for small \(y_i\).

### 1.5 R-Squared (\( R^2 \))
\[
R^2 = 1 - \frac{\sum_{i=1}^{N} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{N} (y_i - \bar{y})^2}
\]
- **Interpretation:** Compares the performance of the model against a baseline that always predicts the mean \(\bar{y}\).  
- **Pros:** Commonly used, easy to interpret. A value near 1 indicates the model explains most of the variance; near 0 means it’s no better than predicting the mean. Negative values indicate it’s worse than the mean predictor.  
- **Cons:** Doesn’t provide a complete picture of the error magnitudes. Doesn’t distinguish between over- and under-prediction.

### 1.6 Adjusted R-Squared
\[
\text{Adjusted } R^2 = 1 - \left(1 - R^2 \right) \frac{N - 1}{N - p - 1}
\]
- **Interpretation:** Similar to \( R^2 \) but penalizes the model for having too many predictors \(p\).  
- **Pros:** More robust than simple \( R^2 \) when comparing models with different numbers of features.  
- **Cons:** Still doesn’t quantify error magnitude like MSE or MAE.

### 1.7 Huber Loss (Less Common as a Final Metric, More a Training Loss)
- **Definition:** Combines MSE for small errors and MAE for large errors, making it robust to outliers.  
- **Use:** Typically employed as a training loss rather than a post-hoc evaluation metric.

---

## 2. Qualitative and Diagnostic Methods

### 2.1 Residual Analysis
- **Idea:** Residuals (\(y_i - \hat{y}_i\)) should look like random noise with no clear pattern when plotted against predicted values or inputs.  
- **What to Look For:**  
  - Any patterns or trends in the residuals may indicate the model has missed certain relationships.  
  - Residuals with increasing variance (i.e., “fan shape”) suggest heteroscedasticity.  
  - Non-zero mean of residuals suggests the model is biased.

### 2.2 Visual Inspection
- **Scatter Plot of Predictions vs. Actuals:**  
  - If the points cluster around the diagonal (45° line), the predictions match the actuals well.  
- **Distribution of Errors (Residuals):**  
  - Ideally, errors should be centered around zero and roughly normally distributed for many regression assumptions.

### 2.3 Cross-Validation
- **K-Fold Cross-Validation:**  
  - Splits the dataset into \(k\) folds and trains the model on \(k-1\) folds while validating on the remaining fold.  
  - The average performance across folds gives a more robust estimate of generalization.  
- **Pros:** Makes efficient use of data, especially if the dataset is small.  
- **Cons:** Computationally more expensive than a single train/test split.

### 2.4 Domain-Specific Benchmarks
- **Context Matters:**  
  - In some industries, an error of 2 units might be significant, while in others, 2 units might be negligible.  
- **Compare Against a Baseline Model:**  
  - A naive model that always predicts the mean or median can be a baseline for how well your sophisticated model is doing.

---

## 3. Choosing the Right Metric

1. **Interpretability and Use Case:**  
   - If you care about large errors disproportionately, **MSE/RMSE** is useful.  
   - If you want a single “average” measure of error magnitude, **MAE** works well.  
   - If you’re interested in relative error, **MAPE** is intuitive.

2. **Scale of the Target Variable:**  
   - **RMSE** uses the same units as the target, which can aid in interpretation.  
   - **MAPE** offers a percentage-based interpretation but can be misleading near zero values.

3. **Comparison or Model Selection:**  
   - **\( R^2 \) or Adjusted \( R^2 \)** are common when comparing multiple models on the same dataset.  
   - **Cross-validation** plus a chosen metric provides a more reliable view of performance.

4. **Outliers and Data Distribution:**  
   - **MAE** or **Huber**-style metrics handle outliers better than pure MSE.

---

## 4. Summary

- **MSE, RMSE, MAE, MAPE, and \(R^2\)** are the most frequently used metrics in regression.  
- **Residual Analysis** and **visual inspection** are crucial for diagnosing how well the model is capturing the data’s structure.  
- **Cross-validation** is recommended to ensure the model generalizes, rather than just performing well on a single train/test split.  
- Always choose metrics and evaluation methods that align with your **business goal** or **application context**—there is no one-size-fits-all approach.

By combining **quantitative metrics** with **qualitative residual and visual analysis**, you can gain a comprehensive understanding of your model’s strengths, weaknesses, and readiness for real-world deployment.
