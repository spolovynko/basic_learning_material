# CONTENT

- [ARCHITECTURE](#architecture)

<a id='architecture'></a>
# ARCHITECTURE

The word *typical* is on purpose.

Because the architecture of a classification neural network can vary widely depending on the problem you’re working on (e.g. images, text, tabular data). However, there are some fundamental components you’ll almost always see:

* An **input layer** receiving your features (the data).
* One or more **hidden layers** for learning representations of the data.
* An **output layer** for producing predictions (class probabilities).

Much of the rest is up to the data practitioner and the nature of the dataset.

Below are some **standard values** you’ll often use in your classification neural networks.

| **Hyperparameter**           | **Binary Classification**                                                                                                                                                                               | **Multiclass Classification**                                                                                                                                                                           |
|------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Input layer shape**        | Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction)                                                                                           | Same as binary classification                                                                                                                                                                           |
| **Hidden layer(s)**          | Problem specific, minimum = 1, maximum = unlimited. Often 1–3 hidden layers are a good starting point                                                                                                   | Same as binary classification                                                                                                                                                                           |
| **Neurons per hidden layer** | Problem specific, generally 10 to 100. Too few → underfitting, too many → overfitting (if not regularized)                                                                                             | Same as binary classification                                                                                                                                                                           |
| **Output layer shape**       | 1 neuron if using a Sigmoid activation (represents the probability of the “positive” class)                                                                                                              | 1 neuron per class (e.g. 3 neurons for 3 classes). Each neuron represents a class probability when using Softmax                                                                                       |
| **Hidden activation**        | Usually [ReLU](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning) (Rectified Linear Unit) for fast convergence and to avoid vanishing gradients                           | Same as binary classification                                                                                                                                                                           |
| **Output activation**        | [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function), outputs values in [0, 1], interpretable as a probability                                                                                     | [Softmax](https://en.wikipedia.org/wiki/Softmax_function), outputs a probability distribution across all classes                                                                                        |
| **Loss function**            | [Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression). In TensorFlow, [`tf.keras.losses.BinaryCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy) | Cross entropy as well, but in TensorFlow typically [`tf.keras.losses.CategoricalCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) or `SparseCategoricalCrossentropy` depending on labels |
| **Optimizer**                | [SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD) (stochastic gradient descent) or [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)                | Same as binary classification                                                                                                                                                                           |

Table 1: Typical architecture of a classification network. Source: Adapted from page 295 of [Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow Book by Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

---

### Why These Choices?

1. **Input Layer Shape**  
   - Matches your dataset’s features. If you have 5 numeric features (e.g., age, sex, height, weight, smoking status), the input layer needs 5 input nodes.

2. **Hidden Layers & Neurons**  
   - More hidden layers and/or more neurons allow the network to learn more complex relationships.
   - Rule of thumb: start small (1–3 hidden layers, 10–100 neurons each) and adjust based on performance and over/underfitting.

3. **Output Layer Shape**  
   - **Binary Classification**: 1 neuron with a **Sigmoid** activation outputs a probability in \([0, 1]\). A value above 0.5 is typically interpreted as the “positive” class.  
   - **Multiclass Classification**: 1 neuron per class with a **Softmax** activation. Each neuron’s output can be interpreted as the probability of belonging to that particular class, and all these probabilities sum to 1.

4. **Hidden Layer Activation**  
   - [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) is a common default for hidden layers because it helps with vanishing gradients and trains efficiently.

5. **Output Activation**  
   - **Sigmoid** for binary classification yields a single probability.  
   - **Softmax** for multiclass yields a probability distribution across all possible classes.

6. **Loss Function**  
   - **BinaryCrossentropy** is used for binary classification (compares predicted probability to the actual 0/1 label).  
   - **CategoricalCrossentropy** (or **SparseCategoricalCrossentropy** if labels are integer-encoded) is used for multiclass classification.

7. **Optimizer**  
   - **SGD** is simpler but may require careful tuning of learning rate; can yield excellent results, especially if combined with momentum.  
   - **Adam** adapts the learning rates automatically, often converges faster, and is a popular choice as a starting optimizer.

---

### Getting Started

Don’t worry if not much of the above makes sense right now; a lot becomes clearer when you **practice** training models and tweaking parameters. In practice, you’ll:

1. **Load & preprocess data** (scale numeric features, encode categorical variables, etc.).
2. **Build your network** (decide on the number of layers, neurons, activation functions, etc.).
3. **Compile the model** with the appropriate loss function (e.g., binary crossentropy) and optimizer (e.g., SGD or Adam).
4. **Train the model** on your dataset while monitoring **loss** and **accuracy** (or another classification metric).
5. **Evaluate the model** on a validation or test set, and iterate on your design.

By adjusting these “typical” hyperparameters, you can adapt the classification neural network to your specific dataset—whether it’s binary or multiclass. Once you gain more experience, you can experiment with additional layers, more sophisticated regularization (e.g., dropout, batch normalization), and advanced optimizers to further improve performance.
