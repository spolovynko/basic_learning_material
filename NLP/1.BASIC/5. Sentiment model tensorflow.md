- [Preprocessing](#preprocessing)
- [Building dataset](#dataset)
- [Building a tensorflow model and save](#tensorflow)
- [Loading and predicitons](#predictions)


<a id='preprocessing'></a>
# NLP Preprocessing for TensorFlow

## Introduction
Natural Language Processing (NLP) involves various preprocessing steps to prepare textual data for machine learning models, particularly when using TensorFlow. Preprocessing ensures data quality, reduces noise, and improves model performance.

## Key Preprocessing Steps
### 1. Text Cleaning
Before feeding text into a model, it is essential to clean it by:
- Removing special characters and punctuation
- Converting text to lowercase
- Eliminating extra whitespaces
- Expanding contractions (e.g., "don't" → "do not")
- Removing stopwords (e.g., "the", "and", "is")

Example using Python:
```python
import re
import string
import tensorflow as tf

text = "Hello World! This is an NLP Preprocessing example."
text = text.lower()
text = re.sub(f"[{string.punctuation}]", "", text)
print(text)  # Output: hello world this is an nlp preprocessing example
```

### 2. Tokenization
Tokenization splits text into individual units, such as words or subwords.

Using TensorFlow's `Tokenizer`:
```python
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])
tokens = tokenizer.texts_to_sequences([text])
print(tokens)  # Output: [[2, 3, 4, 5, 6, 7, 8, 9]]
```

### 3. Text Vectorization
TensorFlow provides efficient methods to transform text into numerical data, such as:

#### 3.1 One-Hot Encoding
```python
one_hot = tf.keras.preprocessing.text.one_hot(text, 50)
print(one_hot)
```

#### 3.2 Word Embeddings
Word embeddings capture semantic meaning:
```python
from tensorflow.keras.layers import Embedding
import numpy as np

embedding_layer = Embedding(input_dim=50, output_dim=8, input_length=10)
input_text = np.array(tokens)
output = embedding_layer(input_text)
print(output.shape)  # Output: (1, 9, 8)
```

### 4. Padding Sequences
To standardize input lengths, we pad or truncate sequences:
```python
from tensorflow.keras.preprocessing.sequence import pad_sequences

padded_tokens = pad_sequences(tokens, maxlen=10, padding='post')
print(padded_tokens)
```

### 5. Stemming and Lemmatization
- **Stemming:** Reduces words to their root form (e.g., "running" → "run")
- **Lemmatization:** Converts words to their base form (e.g., "better" → "good")

Using `nltk`:
```python
from nltk.stem import PorterStemmer, WordNetLemmatizer

stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()
print(stemmer.stem("running"))  # Output: run
print(lemmatizer.lemmatize("better", pos="a"))  # Output: good
```

## Conclusion
Preprocessing is a vital step in NLP for TensorFlow, ensuring high-quality input for models. By cleaning, tokenizing, vectorizing, padding, and normalizing text, we can improve model performance significantly. TensorFlow provides powerful tools to automate these steps efficiently.


<a id='dataset'></a>
# Building a Dataset for NLP in TensorFlow

## Introduction
Creating a high-quality dataset is crucial for training Natural Language Processing (NLP) models in TensorFlow. This process involves collecting, cleaning, labeling, and transforming text data into a format suitable for deep learning models.

## Steps to Build an NLP Dataset

### 1. Data Collection
The first step is to gather textual data from various sources:
- **Public Datasets**: Use open datasets like IMDB, Yelp reviews, and Common Crawl.
- **Web Scraping**: Extract data from websites using libraries like `BeautifulSoup` and `Scrapy`.
- **APIs**: Retrieve data from APIs like Twitter, Reddit, and Google News.

Example using `requests`:
```python
import requests
url = "https://api.example.com/data"
response = requests.get(url)
data = response.json()
```

### 2. Data Cleaning
Textual data often contains noise that needs to be removed. Common cleaning steps include:
- **Removing special characters and punctuation**
- **Lowercasing text**
- **Eliminating stopwords**
- **Correcting spelling errors**

Example:
```python
import re
text = "This is a sample TEXT!"
cleaned_text = re.sub(r'[^a-zA-Z0-9 ]', '', text.lower())
print(cleaned_text)  # Output: this is a sample text
```

### 3. Data Labeling
For supervised learning, text data must be labeled. Labeling methods include:
- **Manual Labeling**: Annotate text manually using tools like Label Studio.
- **Automated Labeling**: Use pre-trained models to generate labels.
- **Crowdsourcing**: Platforms like Amazon Mechanical Turk help distribute labeling tasks.

### 4. Tokenization
Convert text into numerical representations:
```python
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(["Hello world! TensorFlow NLP."])
tokens = tokenizer.texts_to_sequences(["Hello world!"])
print(tokens)  # Output: [[1, 2]]
```

### 5. Creating a TensorFlow Dataset
Convert the processed text into a TensorFlow dataset:
```python
import tensorflow as tf

text_data = ["This is a sentence", "Another text sample"]
labels = [0, 1]
dataset = tf.data.Dataset.from_tensor_slices((text_data, labels))
```

### 6. Text Vectorization
Use TensorFlow's `TextVectorization` layer to convert text into numerical format:
```python
vectorizer = tf.keras.layers.TextVectorization(output_mode='int')
vectorizer.adapt(text_data)
vectorized_text = vectorizer(text_data)
```

### 7. Padding and Batching
Ensure that all text sequences have the same length:
```python
from tensorflow.keras.preprocessing.sequence import pad_sequences

padded_sequences = pad_sequences(tokens, maxlen=10, padding='post')
```
Batching the dataset:
```python
dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)
```

## Conclusion
Building an NLP dataset in TensorFlow involves collecting, cleaning, labeling, tokenizing, and transforming text data into a structured format. Using TensorFlow's dataset and preprocessing tools, we can create efficient and scalable pipelines for training NLP models.


<a id='tensorflow'></a>
# Building, Training, and Saving a TensorFlow Model

## Introduction
Building a machine learning model in TensorFlow involves defining the architecture, compiling the model, training it on data, and saving it for future use. This guide covers each step with practical examples.

## 1. Defining the Model Architecture

TensorFlow provides the `Sequential` API for building models easily.

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Define the model
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(100,)),
    layers.Dense(64, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])
```

## 2. Compiling the Model
Compiling specifies the loss function, optimizer, and metrics used during training.

```python
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
```

## 3. Training the Model
Training requires feeding data into the model with `fit`.

```python
import numpy as np

# Generate synthetic data
X_train = np.random.rand(1000, 100)
y_train = np.random.randint(0, 2, size=(1000,))

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)
```

## 4. Evaluating the Model
After training, evaluate the model using test data.

```python
X_test = np.random.rand(200, 100)
y_test = np.random.randint(0, 2, size=(200,))

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy:.4f}")
```

## 5. Saving and Loading the Model

### Saving the Model
TensorFlow allows saving models in two formats:

- **HDF5 Format** (Single file)

```python
model.save("model.h5")
```

- **SavedModel Format** (Recommended for TensorFlow Serving)

```python
model.save("saved_model")
```

### Loading the Model

To reload the saved model:

```python
# Load from HDF5
model_h5 = keras.models.load_model("model.h5")

# Load from SavedModel format
model_saved = keras.models.load_model("saved_model")
```

## Conclusion
This guide covered defining, training, evaluating, and saving a TensorFlow model. These steps provide a strong foundation for developing and deploying machine learning models effectively.


<a id='predictions'></a>
# Making Predictions on NLP with TensorFlow

## Introduction
Making predictions using a trained NLP model in TensorFlow involves preprocessing input text, passing it through the model, and interpreting the output. This guide covers these steps with practical examples.

## 1. Loading a Trained Model
To make predictions, load a previously saved model.

```python
import tensorflow as tf
from tensorflow import keras

# Load the model
model = keras.models.load_model("saved_model")
```

If the model was saved in HDF5 format:

```python
model = keras.models.load_model("model.h5")
```

## 2. Preprocessing Input Text
Before making predictions, the text must be tokenized and vectorized.

```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Example text input
texts = ["The movie was fantastic!", "I did not enjoy the film."]

# Load tokenizer (assuming it was saved)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)

# Convert texts to sequences
sequences = tokenizer.texts_to_sequences(texts)

# Pad sequences to match model input size
padded_sequences = pad_sequences(sequences, maxlen=100, padding='post')
```

## 3. Making Predictions
Pass the preprocessed text into the model.

```python
predictions = model.predict(padded_sequences)
print(predictions)
```

For a binary classification model (e.g., sentiment analysis):

```python
labels = ["Negative", "Positive"]

for i, pred in enumerate(predictions):
    print(f"Review: {texts[i]} -> Sentiment: {labels[int(pred > 0.5)]}")
```

## 4. Handling Multi-Class Predictions
If the model predicts multiple categories, use `argmax()` to determine the most probable class.

```python
import numpy as np

multi_class_predictions = model.predict(padded_sequences)
predicted_classes = np.argmax(multi_class_predictions, axis=1)
print(predicted_classes)
```

## 5. Using TextVectorization Layer
If the model was trained with a `TextVectorization` layer, ensure it is included during inference.

```python
import tensorflow.keras.layers as layers

# Load model with TextVectorization layer
model = keras.models.load_model("saved_model")
text_vectorizer = model.get_layer("text_vectorization")

# Transform input
vectorized_text = text_vectorizer(texts)
predictions = model.predict(vectorized_text)
print(predictions)
```

## Conclusion
Making predictions in NLP using TensorFlow involves loading a trained model, preprocessing input text, and interpreting model outputs. Proper text preprocessing ensures consistency between training and inference, improving prediction accuracy.

