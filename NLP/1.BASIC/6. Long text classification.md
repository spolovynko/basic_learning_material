- [Using BERT](#bert)
- [Pytorch](#pytorch)

<a id='bert'></a>
# Long Text Classification with BERT

## 1. Introduction
**Long text classification** is a challenging NLP task that involves categorizing lengthy documents, such as **news articles, research papers, legal texts, or customer reviews**. While **BERT (Bidirectional Encoder Representations from Transformers)** is a powerful language model, it has a **512-token limit**, making it difficult to process long texts directly.

This document explores strategies for handling long-text classification using **BERT-based models**, including techniques like **chunking, hierarchical attention, and Longformer**.

---

## 2. Challenges in Long Text Classification
### **2.1 Token Limitations**
- **BERT has a maximum token limit of 512**, which includes special tokens (`[CLS]` and `[SEP]`).
- Long documents **get truncated**, potentially losing valuable information.

### **2.2 Computational Constraints**
- Processing large text chunks **increases memory requirements**.
- Transformer-based models have **quadratic complexity** (`O(n^2)`) for attention mechanisms.

### **2.3 Contextual Understanding**
- Simply truncating or summarizing text may **remove key context**, impacting classification accuracy.

---

## 3. Strategies for Handling Long Texts with BERT
Several techniques can be used to classify long texts effectively:

### **3.1 Truncation and Head-Tail Selection**
- Extract important **text segments**, such as **first (head) and last (tail) 256 tokens**.
- Example:
  ```python
  tokens = tokenizer(text, max_length=512, truncation=True, return_tensors='pt')
  ```
- **Pros**: Simple and efficient.
- **Cons**: Risk of missing **important middle sections** of the text.

### **3.2 Text Chunking (Sliding Window)**
- **Divide long text** into overlapping chunks (e.g., **stride of 256 tokens**).
- Pass **each chunk separately** through BERT and aggregate outputs.
- Example:
  ```python
  def chunk_text(text, tokenizer, chunk_size=256, stride=128):
      tokens = tokenizer.encode(text, truncation=False)
      chunks = [tokens[i:i+chunk_size] for i in range(0, len(tokens), stride)]
      return chunks
  ```
- **Pros**: Retains more context than truncation.
- **Cons**: Increases computation and requires **aggregation strategies**.

### **3.3 Hierarchical BERT (HBERT)**
- Uses **two-stage encoding**:
  - **Sentence-level BERT** extracts embeddings for individual sentences.
  - **Document-level BERT** aggregates sentence embeddings for final classification.
- **Pros**: Preserves hierarchical structure.
- **Cons**: Requires additional training.

### **3.4 Using Longformer or BigBird**
- **Longformer** and **BigBird** are transformer models designed for **long texts**.
- They **replace full attention with sparse attention**, reducing complexity to `O(n log n)`.
- Example:
  ```python
  from transformers import LongformerForSequenceClassification
  model = LongformerForSequenceClassification.from_pretrained("allenai/longformer-base-4096")
  ```
- **Pros**: Handles up to **4,096 tokens**.
- **Cons**: Requires **fine-tuning** on domain-specific data.

---

## 4. Implementing Long Text Classification with BERT
### **Step 1: Load Pre-trained BERT Model**
```python
from transformers import BertTokenizer, BertForSequenceClassification

model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)
```

### **Step 2: Preprocess Long Text (Chunking Strategy)**
```python
def preprocess_long_text(text, tokenizer, max_length=512, stride=256):
    tokens = tokenizer.encode_plus(text, max_length=max_length, truncation=True, return_tensors="pt")
    return tokens
```

### **Step 3: Pass Chunks Through BERT and Aggregate**
```python
import torch

def classify_long_text(text, model, tokenizer):
    chunks = chunk_text(text, tokenizer)
    predictions = []
    for chunk in chunks:
        inputs = torch.tensor([chunk])
        outputs = model(inputs).logits
        predictions.append(outputs)
    return torch.mean(torch.stack(predictions), dim=0)  # Aggregate logits
```

### **Step 4: Evaluate Model Performance**
```python
from sklearn.metrics import accuracy_score

def evaluate_model(model, tokenizer, dataset):
    predictions, labels = [], []
    for text, label in dataset:
        pred = classify_long_text(text, model, tokenizer).argmax().item()
        predictions.append(pred)
        labels.append(label)
    return accuracy_score(labels, predictions)
```

---

## 5. Choosing the Right Model for Long Text Classification
| **Approach**              | **Pros**                                     | **Cons**                                        |
|--------------------------|--------------------------------|--------------------------------|
| **Truncation (Head-Tail)** | Simple, low computation        | May lose important middle context |
| **Text Chunking**         | Retains more context           | Requires aggregation strategy |
| **Hierarchical BERT**     | Captures sentence relationships | More complex training |
| **Longformer/BigBird**    | Handles up to 4,096 tokens     | Requires fine-tuning |

---

## 6. Conclusion
Classifying long texts with **BERT** requires strategies to **overcome the 512-token limit**. Depending on the use case, options include **chunking, hierarchical BERT, or using models like Longformer**. Choosing the best approach depends on **accuracy requirements, computational resources, and available training data**.

### **Key Takeaways**
- **BERT has a 512-token limit**, requiring preprocessing for long text.
- **Chunking and hierarchical methods help retain context.**
- **Longformer and BigBird offer efficient solutions for extended text.**
- **Fine-tuning on domain-specific data improves classification performance.**


<a id='pytorch'></a>
# PyTorch Usage and Window Method in PyTorch

## 1. Introduction
**PyTorch** is an open-source deep learning framework developed by **Facebook AI Research (FAIR)**. It provides **dynamic computation graphs, GPU acceleration, and an intuitive interface** for building and training machine learning models. 

The **window method** is a technique used in **time-series forecasting, NLP, and signal processing**, where a fixed-size moving window extracts overlapping or non-overlapping sub-sequences from a dataset.

---

## 2. Key Features of PyTorch
- **Dynamic Computation Graphs**: Allows on-the-fly changes to network architecture.
- **Tensor-Based Computation**: Efficient GPU-accelerated tensor operations.
- **Autograd (Automatic Differentiation)**: Simplifies backpropagation.
- **TorchScript**: Converts PyTorch models for deployment.
- **Strong Community Support**: Regular updates and improvements.

---

## 3. Basic Usage of PyTorch
### **3.1 Installing PyTorch**
```bash
pip install torch torchvision torchaudio
```

### **3.2 Creating Tensors**
```python
import torch

# Creating tensors
x = torch.tensor([1.0, 2.0, 3.0])
y = torch.randn(3, 3)  # Random tensor

# Basic operations
z = x + 2
print(z)
```

### **3.3 Using CUDA for GPU Acceleration**
```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
x = torch.tensor([1.0, 2.0, 3.0], device=device)
print(x)
```

---

## 4. The Window Method in PyTorch
The **window method** is used for creating sliding window sub-sequences, often seen in **time-series forecasting, NLP sequence processing, and image processing**.

### **4.1 Why Use the Window Method?**
- Extracts **localized information** from long sequences.
- Enables **context-aware** learning for models.
- Reduces computational overhead compared to full-sequence processing.

### **4.2 Implementing the Window Method in PyTorch**
The window method involves **extracting overlapping sequences** from a time-series or text input using **tensor indexing**.

#### **4.2.1 Sliding Window on Time-Series Data**
```python
import torch

def create_windows(data, window_size):
    windows = []
    for i in range(len(data) - window_size + 1):
        windows.append(data[i:i + window_size])
    return torch.stack(windows)

# Example time-series data
data = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8])
window_size = 3
windows = create_windows(data, window_size)
print(windows)
```

#### **4.2.2 Using `unfold()` for Efficient Sliding Window**
PyTorch provides an efficient built-in function **`unfold()`** for extracting sliding windows:
```python
sequence = torch.arange(1, 9).float()  # Example sequence
sequence = sequence.unsqueeze(0)  # Add batch dimension

# Extract sliding windows
windows = sequence.unfold(dimension=1, size=3, step=1)
print(windows)
```

#### **4.2.3 Applying Window Method in NLP**
For **language models**, the window method helps in generating **fixed-size context windows**:
```python
text = "PyTorch is amazing for deep learning."
tokens = text.split()
window_size = 2

# Create n-gram windows
windows = [tokens[i:i+window_size] for i in range(len(tokens)-window_size+1)]
print(windows)
```

---

## 5. Training a Model with Windowed Data
### **5.1 Creating a Dataset Class for Windowed Data**
```python
from torch.utils.data import Dataset, DataLoader

class TimeSeriesDataset(Dataset):
    def __init__(self, data, window_size):
        self.data = data
        self.window_size = window_size
    
    def __len__(self):
        return len(self.data) - self.window_size
    
    def __getitem__(self, idx):
        return self.data[idx:idx+self.window_size], self.data[idx+self.window_size]

# Example usage
data = torch.arange(1, 20).float()
dataset = TimeSeriesDataset(data, window_size=4)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

for x, y in dataloader:
    print(f"Input: {x}, Target: {y}")
```

### **5.2 Training a Simple Model with Sliding Window Data**
```python
import torch.nn as nn

class SimpleModel(nn.Module):
    def __init__(self, input_size):
        super(SimpleModel, self).__init__()
        self.linear = nn.Linear(input_size, 1)
    
    def forward(self, x):
        return self.linear(x)

# Training the model
model = SimpleModel(window_size)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
loss_fn = nn.MSELoss()

for epoch in range(10):
    for x, y in dataloader:
        optimizer.zero_grad()
        output = model(x.float())
        loss = loss_fn(output.squeeze(), y.float())
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")
```

---

## 6. Applications of the Window Method in PyTorch
| **Application**          | **Use Case** |
|--------------------------|-------------|
| **Time-Series Forecasting** | Stock prices, weather prediction |
| **NLP** | Language modeling, text segmentation |
| **Speech Processing** | Audio signal analysis |
| **Image Processing** | Object detection, image segmentation |

---

## 7. Challenges and Optimizations
- **Handling Overlapping Windows**: Optimizing batch processing to reduce redundancy.
- **Computational Efficiency**: Using **`unfold()`** instead of loops for faster processing.
- **Memory Consumption**: Managing GPU memory efficiently when working with large datasets.

---

## 8. Conclusion
The **window method** in PyTorch is a powerful approach for handling **sequence-based tasks** in **time-series forecasting, NLP, and signal processing**. By leveraging **tensor indexing, `unfold()`, and DataLoader**, PyTorch allows efficient and scalable implementation of this technique.

### **Key Takeaways**
- **The window method extracts local context from sequential data.**
- **PyTorch provides efficient tools (`unfold()`, DataLoader) to streamline processing.**
- **Sliding window techniques enhance predictive performance in sequence models.**
- **Training on windowed data enables models to learn time-dependent patterns.**