- [NER](#ner)
- [SpaCy](#spacy)
- [Sentiment](#sentiment)
- [RoBERTa](#roberta)

<a id='ner'></a>
# Named Entity Recognition (NER) in TensorFlow

## Introduction
Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) that identifies and classifies named entities in text, such as persons, organizations, locations, dates, and more. This guide covers how to implement NER using TensorFlow.

## 1. Understanding NER
NER involves detecting words or phrases in a sentence that represent entities and categorizing them into predefined classes. Common entity types include:
- **PER**: Person names (e.g., "Elon Musk")
- **ORG**: Organizations (e.g., "Google")
- **LOC**: Locations (e.g., "New York")
- **DATE**: Dates (e.g., "January 1, 2022")

## 2. Preparing the Dataset
NER datasets typically consist of tokenized sentences with labeled entities. A popular dataset is the **CoNLL-2003** dataset.

Example format:
```
Obama B-PER
was O
born O
in O
Hawaii B-LOC
```

### Loading and Preprocessing Data
```python
import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Example tokenized sentences
sentences = [["Obama", "was", "born", "in", "Hawaii"],
             ["Tesla", "was", "founded", "by", "Elon", "Musk"]]

# Corresponding entity labels (integer encoded)
labels = [[1, 0, 0, 0, 2], [2, 0, 0, 0, 1, 1]]  # 1: PER, 2: LOC, 0: O (outside entity)

# Pad sequences to uniform length
max_len = 10
padded_sentences = pad_sequences(sentences, maxlen=max_len, padding='post', dtype='object')
padded_labels = pad_sequences(labels, maxlen=max_len, padding='post', value=0)
```

## 3. Building the NER Model
A common approach for NER is using an **LSTM-based** model with an **embedding layer** and a **CRF (Conditional Random Field) layer**.

```python
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional
from tensorflow.keras.models import Sequential

vocab_size = 5000  # Adjust based on dataset
embedding_dim = 128
num_classes = 3  # (PER, LOC, O)

# Define model
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),
    Bidirectional(LSTM(64, return_sequences=True)),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
```

## 4. Training the Model
```python
# Example training data (for demonstration purposes)
X_train = np.random.randint(1, vocab_size, (1000, max_len))  # Dummy data
y_train = np.random.randint(0, num_classes, (1000, max_len))

model.fit(X_train, y_train, epochs=5, batch_size=32)
```

## 5. Making Predictions
```python
# Example sentence for prediction
test_sentence = ["Elon", "Musk", "is", "the", "CEO", "of", "Tesla"]

# Convert words to integers using tokenizer (assume it’s trained and loaded)
test_sequence = tokenizer.texts_to_sequences([test_sentence])
test_sequence = pad_sequences(test_sequence, maxlen=max_len, padding='post')

# Predict entities
predictions = model.predict(test_sequence)
predicted_classes = np.argmax(predictions, axis=-1)
print(predicted_classes)
```

## Conclusion
NER in TensorFlow can be implemented using an LSTM-based model with embeddings. Proper data preprocessing, sequence padding, and an effective model architecture are key to achieving good results. More advanced models can incorporate **CRFs, Transformers, or BERT-based architectures** for improved performance.


<a id='spacy'></a>
# Named Entity Recognition (NER) with spaCy

## Introduction
Named Entity Recognition (NER) is a key task in Natural Language Processing (NLP) that involves identifying and classifying entities in text. spaCy is a popular NLP library that provides a robust pre-trained NER model and allows for custom entity recognition.

## 1. Installing and Importing spaCy
To use spaCy for NER, install the required package:
```bash
pip install spacy
```

Download the English model:
```bash
python -m spacy download en_core_web_sm
```

Import spaCy:
```python
import spacy
```

## 2. Using Pre-Trained NER Model
spaCy provides pre-trained models that can identify various entities such as **PERSON, ORG, LOC, DATE, GPE, PRODUCT**, etc.

```python
nlp = spacy.load("en_core_web_sm")
text = "Elon Musk is the CEO of Tesla, headquartered in California."
doc = nlp(text)

for ent in doc.ents:
    print(f"Entity: {ent.text}, Label: {ent.label_}")
```

### Output Example:
```
Entity: Elon Musk, Label: PERSON
Entity: Tesla, Label: ORG
Entity: California, Label: GPE
```

## 3. Customizing NER with spaCy
If the default model does not recognize certain entities, we can **train a custom NER model**.

### 3.1 Adding a New Entity
```python
from spacy.tokens import Span

# Add a new entity manually
doc = nlp("John Doe is the founder of SpaceX.")
ent = Span(doc, 0, 2, label="PERSON")
doc.ents = list(doc.ents) + [ent]

for ent in doc.ents:
    print(f"Entity: {ent.text}, Label: {ent.label_}")
```

### 3.2 Training a Custom NER Model
To teach spaCy a new entity type, fine-tuning is required.

```python
import random
from spacy.training import Example

# Load model and prepare training data
nlp = spacy.load("en_core_web_sm")
ner = nlp.get_pipe("ner")

TRAIN_DATA = [
    ("Apple is looking at buying U.K. startup for $1 billion", {"entities": [(0, 5, "ORG")]}),
    ("Google acquires AI startup in Canada", {"entities": [(0, 6, "ORG"), (24, 30, "GPE")]})
]

# Add new entity labels
ner.add_label("ORG")

# Train the model
optimizer = nlp.resume_training()
for _ in range(10):
    random.shuffle(TRAIN_DATA)
    for text, annotations in TRAIN_DATA:
        example = Example.from_dict(nlp.make_doc(text), annotations)
        nlp.update([example], drop=0.5, sgd=optimizer)

# Test the trained model
doc = nlp("Apple is acquiring an AI company.")
for ent in doc.ents:
    print(f"Entity: {ent.text}, Label: {ent.label_}")
```

## 4. Visualizing NER Results
spaCy provides `displacy` for visualization.

```python
from spacy import displacy

doc = nlp("Microsoft announced a partnership with OpenAI in San Francisco.")
displacy.render(doc, style="ent", jupyter=True)
```

## Conclusion
spaCy provides powerful tools for both pre-trained and custom NER tasks. It supports efficient entity recognition, customization, and visualization, making it a valuable tool for NLP applications.


<a id='sentiment'></a>
# Named Entity Recognition (NER) with Sentiment Analysis

## Introduction
Named Entity Recognition (NER) and Sentiment Analysis are two key NLP tasks. **NER** identifies entities in text (e.g., names, locations, organizations), while **Sentiment Analysis** determines the sentiment associated with text (e.g., positive, negative, neutral). Combining these techniques allows for extracting meaningful insights from text data, such as identifying sentiment around specific entities.

## 1. Installing and Importing Dependencies
Ensure you have the necessary libraries installed:
```bash
pip install spacy textblob
```

Import required libraries:
```python
import spacy
from textblob import TextBlob
```

## 2. Using Pre-Trained NER Model
spaCy provides pre-trained models for entity recognition.

```python
# Load spaCy model
nlp = spacy.load("en_core_web_sm")

# Example text
text = "Elon Musk announced that Tesla's stock is rising, which is great news for investors."
doc = nlp(text)

# Extract entities
entities = [(ent.text, ent.label_) for ent in doc.ents]
print("Entities:", entities)
```

### Sample Output:
```
Entities: [('Elon Musk', 'PERSON'), ('Tesla', 'ORG')]
```

## 3. Performing Sentiment Analysis
Use TextBlob to analyze sentiment.

```python
def get_sentiment(text):
    sentiment = TextBlob(text).sentiment.polarity
    if sentiment > 0:
        return "Positive"
    elif sentiment < 0:
        return "Negative"
    else:
        return "Neutral"

# Get sentiment
sentiment = get_sentiment(text)
print("Overall Sentiment:", sentiment)
```

### Sample Output:
```
Overall Sentiment: Positive
```

## 4. Linking NER with Sentiment
Now, we extract sentiments for each entity in the text.

```python
def analyze_entity_sentiment(text):
    doc = nlp(text)
    entity_sentiments = {}
    
    for ent in doc.ents:
        entity_text = ent.text
        entity_sentiments[entity_text] = get_sentiment(entity_text)
    
    return entity_sentiments

# Get entity-wise sentiment
entity_sentiments = analyze_entity_sentiment(text)
print("Entity-wise Sentiment:", entity_sentiments)
```

### Sample Output:
```
Entity-wise Sentiment: {'Elon Musk': 'Neutral', 'Tesla': 'Positive'}
```

## 5. Advanced: Sentence-Level Entity Sentiment
A more refined approach is to analyze sentiment within sentences containing the entity.

```python
def analyze_entity_sentiment_by_sentence(text):
    doc = nlp(text)
    entity_sentiments = {}
    
    for sent in doc.sents:
        sent_text = sent.text
        sentiment = get_sentiment(sent_text)
        for ent in sent.ents:
            entity_sentiments[ent.text] = sentiment
    
    return entity_sentiments

# Analyze sentiment per sentence containing the entity
entity_sentiments = analyze_entity_sentiment_by_sentence(text)
print("Entity Sentiments by Sentence:", entity_sentiments)
```

### Sample Output:
```
Entity Sentiments by Sentence: {'Elon Musk': 'Positive', 'Tesla': 'Positive'}
```

## Conclusion
Combining **NER** and **Sentiment Analysis** allows for extracting meaningful insights, such as understanding how people feel about specific entities. This technique is useful for **brand monitoring, financial analysis, and customer feedback processing**.


<a id='roberta'></a>
# Named Entity Recognition (NER) with RoBERTa

## Introduction
Named Entity Recognition (NER) using **RoBERTa**, a variant of BERT, is a powerful approach for identifying entities such as **persons, organizations, locations, dates, and more**. RoBERTa improves upon BERT by using more training data and removing the next-sentence prediction objective, making it highly effective for NLP tasks, including NER.

This guide explains how to implement **NER with RoBERTa** using the **Hugging Face Transformers library**.

## 1. Installing Dependencies
Ensure you have the required packages installed:
```bash
pip install transformers datasets torch
```

Import necessary libraries:
```python
import torch
from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline
```

## 2. Loading a Pre-Trained RoBERTa Model for NER
RoBERTa can be fine-tuned for NER using models available on Hugging Face.

```python
# Load pre-trained RoBERTa model and tokenizer
model_name = "Jean-Baptiste/roberta-large-ner-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)

# Initialize NER pipeline
nlp_ner = pipeline("ner", model=model, tokenizer=tokenizer)
```

## 3. Performing Named Entity Recognition
```python
text = "Elon Musk is the CEO of Tesla, which is headquartered in Palo Alto."
results = nlp_ner(text)

# Display results
for entity in results:
    print(f"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}")
```

### Sample Output:
```
Entity: Elon Musk, Label: PER, Score: 0.9985
Entity: Tesla, Label: ORG, Score: 0.9978
Entity: Palo Alto, Label: LOC, Score: 0.9962
```

## 4. Handling Subword Tokenization
RoBERTa uses **subword tokenization**, meaning some words may be split. The following approach ensures correctly formatted output:

```python
def process_ner_results(results):
    entities = []
    current_entity = ""
    current_label = ""
    
    for entity in results:
        word = entity["word"].replace("▁", "")  # Remove special tokens
        label = entity["entity_group"]
        
        if label == current_label:
            current_entity += " " + word  # Merge subwords
        else:
            if current_entity:
                entities.append((current_entity, current_label))
            current_entity = word
            current_label = label
    
    if current_entity:
        entities.append((current_entity, current_label))
    
    return entities

processed_entities = process_ner_results(results)
print("Processed Entities:", processed_entities)
```

### Sample Output:
```
Processed Entities: [('Elon Musk', 'PER'), ('Tesla', 'ORG'), ('Palo Alto', 'LOC')]
```

## 5. Fine-Tuning RoBERTa for Custom NER Tasks
To fine-tune RoBERTa on a custom dataset, use the **Hugging Face Trainer API** with a labeled dataset.

### Loading a Custom Dataset
```python
from datasets import load_dataset

dataset = load_dataset("conll2003")
print(dataset["train"][0])
```

### Fine-Tuning RoBERTa
```python
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./ner_roberta",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"]
)

trainer.train()
```

## Conclusion
RoBERTa is a powerful transformer-based model for **Named Entity Recognition (NER)**. Using the **Hugging Face Transformers library**, we can leverage pre-trained models or fine-tune them on custom datasets. This approach improves accuracy for entity extraction in real-world applications like **financial analysis, healthcare, and social media monitoring**.

