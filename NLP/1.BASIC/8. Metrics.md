- [Accuracy, precision, recall, F1](#basic)
- [ROUGE](#rouge)
- [LCS](#lcs)


<a id='basic'></a>
# Evaluation Metrics in NLP: Precision, Recall, F1-Score, and Accuracy

## Introduction
In Natural Language Processing (NLP), evaluating model performance is crucial to understanding how well a model classifies text. Common evaluation metrics include:
- **Precision**
- **Recall**
- **F1-Score**
- **Accuracy**

These metrics are particularly important in **classification tasks**, including **sentiment analysis, named entity recognition (NER), and text classification**.

---

## 1. Accuracy
**Definition:**
Accuracy measures the proportion of correctly classified instances over the total instances.

\[
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
\]

- **TP (True Positive)**: Correctly classified positive instances
- **TN (True Negative)**: Correctly classified negative instances
- **FP (False Positive)**: Incorrectly classified negative instances as positive
- **FN (False Negative)**: Incorrectly classified positive instances as negative

**Example:**
If a sentiment analysis model classifies 100 reviews and gets 90 correct, the accuracy is:
```python
accuracy = (90 / 100) * 100  # 90%
print("Accuracy:", accuracy)
```

**Limitation:** Accuracy can be misleading if the dataset is imbalanced (e.g., 90% positive and 10% negative). Precision, recall, and F1-score provide better insights.

---

## 2. Precision
**Definition:**
Precision measures how many instances classified as positive are actually positive.

\[
Precision = \frac{TP}{TP + FP}
\]

**Example:**
If an NER model identifies 20 organizations but 5 are incorrect, the precision is:
```python
precision = (15 / 20) * 100  # 75%
print("Precision:", precision)
```

**Use Case:** Useful when **false positives** are costly, such as in medical diagnoses.

---

## 3. Recall
**Definition:**
Recall (Sensitivity) measures how many actual positive instances are correctly identified.

\[
Recall = \frac{TP}{TP + FN}
\]

**Example:**
If there are 30 organizations in the dataset but the model detects only 15, the recall is:
```python
recall = (15 / 30) * 100  # 50%
print("Recall:", recall)
```

**Use Case:** Important when **false negatives** are costly, such as in fraud detection.

---

## 4. F1-Score
**Definition:**
The F1-score is the **harmonic mean** of precision and recall, balancing both metrics.

\[
F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
\]

**Example Calculation:**
```python
precision = 0.75  # 75%
recall = 0.50  # 50%
f1_score = 2 * (precision * recall) / (precision + recall)
print("F1-Score:", f1_score)
```

**Use Case:** Useful when both **false positives and false negatives** matter, such as in search engines.

---

## 5. Implementing in Python
Use `sklearn.metrics` to compute these metrics efficiently:

```python
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

# Example true and predicted labels
y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]
y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 1]

print("Accuracy:", accuracy_score(y_true, y_pred))
print("Precision:", precision_score(y_true, y_pred))
print("Recall:", recall_score(y_true, y_pred))
print("F1-Score:", f1_score(y_true, y_pred))
```

---

## Conclusion
- **Accuracy** is useful in balanced datasets but misleading in imbalanced cases.
- **Precision** focuses on reducing false positives.
- **Recall** emphasizes reducing false negatives.
- **F1-Score** provides a balance between precision and recall.

Choosing the right metric depends on the problem domain, ensuring an optimal model evaluation strategy.
<a id='rouge'></a>
# ROUGE Metrics in NLP

## Introduction
ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate **text summarization, machine translation, and text generation models** by comparing their output against reference texts.

ROUGE measures **n-gram overlap**, **word sequences**, and **longest common subsequence (LCS)** between the generated text and the reference text.

---

## 1. Types of ROUGE Metrics
### 1.1 ROUGE-N (N-gram Overlap)
**ROUGE-N** calculates the overlap of **n-grams** between the reference and generated text.

\[
ROUGE-N = \frac{\sum_{s \in Reference} Count_{match}(s)}{\sum_{s \in Reference} Count(s)}
\]

Common values:
- **ROUGE-1**: Unigram (single word) overlap
- **ROUGE-2**: Bigram (two-word) overlap
- **ROUGE-3**: Trigram (three-word) overlap

Example:
Reference: *"The cat sat on the mat."*  
Generated: *"The cat is on the mat."*  

ROUGE-1 Score (unigram overlap):
```
Matched unigrams: ["The", "cat", "on", "the", "mat"]
ROUGE-1 = 5 / 6 = 0.8333 (83.3%)
```

---

### 1.2 ROUGE-L (Longest Common Subsequence - LCS)
**ROUGE-L** measures the **longest common sequence of words** between the reference and generated text, capturing fluency and sentence structure.

Formula:
\[
ROUGE-L = \frac{LCS(Reference, Generated)}{Length(Reference)}
\]

Example:
Reference: *"The quick brown fox jumps over the lazy dog."*  
Generated: *"The fast brown fox jumped over a lazy dog."*  
LCS = *"The brown fox over the lazy dog"*  

ROUGE-L Score:
```
ROUGE-L = 6 / 9 = 0.6667 (66.7%)
```

---

### 1.3 ROUGE-W (Weighted LCS)
ROUGE-W assigns higher weights to longer consecutive matches rather than individual word matches, improving on ROUGE-L.

---

### 1.4 ROUGE-S (Skip-Bigram)
**ROUGE-S** measures bigram matches that allow for words in between, providing a more flexible matching approach.

Example:
Reference: *"The cat sat on the mat."*  
Generated: *"The cat is on the mat."*  
Skip-bigram matches: *"The on", "cat mat"*

---

## 2. Implementing ROUGE in Python
ROUGE can be computed using the **`rouge-score`** or **`evaluate`** library.

### 2.1 Installing Dependencies
```bash
pip install rouge-score
```

### 2.2 Computing ROUGE Scores
```python
from rouge_score import rouge_scorer

# Define reference and generated text
reference = "The cat sat on the mat."
generated = "The cat is on the mat."

# Initialize ROUGE scorer
scorer = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)
scores = scorer.score(reference, generated)

print("ROUGE Scores:", scores)
```

### Sample Output:
```
ROUGE Scores: {
  'rouge1': Score(precision=0.8333, recall=0.8333, fmeasure=0.8333),
  'rouge2': Score(precision=0.6667, recall=0.6667, fmeasure=0.6667),
  'rougeL': Score(precision=0.6667, recall=0.6667, fmeasure=0.6667)
}
```

---

## 3. Interpreting ROUGE Scores
- **Higher scores** indicate better alignment between the generated and reference text.
- **F-measure** is the most common metric used to balance precision and recall.
- **Lower ROUGE scores** indicate missing key information or structural mismatches.

---

## Conclusion
ROUGE is a widely used metric for evaluating text generation models. It provides useful insights into **content overlap and fluency** between generated and reference texts. While **ROUGE-1 and ROUGE-2** measure **content similarity**, **ROUGE-L** captures **sentence structure** and **fluency**.


<a id='lcs'></a>
# Longest Common Subsequence (LCS)

## Introduction
The **Longest Common Subsequence (LCS)** is a classic algorithmic problem that finds the longest sequence of characters that appear in two given sequences in the same order but not necessarily consecutively. It is widely used in **bioinformatics, text similarity, diff tools, and NLP evaluation metrics like ROUGE-L**.

---

## 1. Definition and Formula
Given two sequences **X** and **Y**, the LCS is the longest sequence **Z** such that **Z** appears as a subsequence in both **X** and **Y**.

### Example:
```
X = "ABCBDAB"
Y = "BDCAB"
LCS(X, Y) = "BCAB"
```

### Recursive Formula:
Let **L(i, j)** be the LCS length for substrings **X[0:i]** and **Y[0:j]**:
- If `X[i] == Y[j]`, then:
  \[
  L(i, j) = 1 + L(i-1, j-1)
  \]
- Otherwise:
  \[
  L(i, j) = \max(L(i-1, j), L(i, j-1))
  \]
- Base Case: **L(0, j) = L(i, 0) = 0** (empty string case)

---

## 2. Implementing LCS in Python

### 2.1 Recursive Approach (Exponential Time Complexity)
```python
def lcs_recursive(X, Y, m, n):
    if m == 0 or n == 0:
        return 0
    elif X[m - 1] == Y[n - 1]:
        return 1 + lcs_recursive(X, Y, m - 1, n - 1)
    else:
        return max(lcs_recursive(X, Y, m - 1, n), lcs_recursive(X, Y, m, n - 1))

X = "ABCBDAB"
Y = "BDCAB"
print("LCS Length:", lcs_recursive(X, Y, len(X), len(Y)))
```

### 2.2 Dynamic Programming Approach (Optimal O(m*n) Time Complexity)
```python
def lcs_dp(X, Y):
    m, n = len(X), len(Y)
    dp = [[0] * (n + 1) for _ in range(m + 1)]

    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if X[i - 1] == Y[j - 1]:
                dp[i][j] = 1 + dp[i - 1][j - 1]
            else:
                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])

    return dp[m][n]

print("LCS Length:", lcs_dp(X, Y))
```

---

## 3. Finding the LCS Sequence
To reconstruct the LCS sequence from the DP table:
```python
def lcs_sequence(X, Y):
    m, n = len(X), len(Y)
    dp = [[0] * (n + 1) for _ in range(m + 1)]

    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if X[i - 1] == Y[j - 1]:
                dp[i][j] = 1 + dp[i - 1][j - 1]
            else:
                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])

    # Backtrack to find the LCS string
    i, j = m, n
    lcs_str = []
    while i > 0 and j > 0:
        if X[i - 1] == Y[j - 1]:
            lcs_str.append(X[i - 1])
            i -= 1
            j -= 1
        elif dp[i - 1][j] > dp[i][j - 1]:
            i -= 1
        else:
            j -= 1

    return "".join(reversed(lcs_str))

print("LCS Sequence:", lcs_sequence(X, Y))
```

### Sample Output:
```
LCS Length: 4
LCS Sequence: BCAB
```

---

## 4. Applications of LCS
### 4.1 Text Similarity & Diff Tools
- Used in `diff` commands to detect changes between two versions of text.

### 4.2 Bioinformatics (DNA Sequence Matching)
- Helps identify common gene sequences.

### 4.3 NLP Evaluation (ROUGE-L Score)
- Used in **text summarization evaluation** to measure similarity between generated and reference summaries.

```python
from rouge_score import rouge_scorer
scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)
scores = scorer.score("The cat is on the mat", "The cat sat on the mat")
print(scores)
```

### Sample Output:
```
{'rougeL': Score(precision=0.8333, recall=0.8333, fmeasure=0.8333)}
```

---

## Conclusion
- **LCS** finds the longest common ordered sequence of two strings.
- **Dynamic programming** provides an efficient solution in O(m*n) time.
- **Useful for NLP tasks**, bioinformatics, and text comparison.

LCS is a foundational algorithm with broad applications in text processing and computational biology.

